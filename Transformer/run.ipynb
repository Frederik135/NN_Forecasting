{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from config import num_epochs, learning_rate, wandb_config, model\n",
    "from cyclical_preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "\n",
    "class StockPredictionModule(pl.LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        predictions, actuals = [], []\n",
    "        for seqs, labels in self.test_loader:\n",
    "            seqs, labels = seqs.to(self.device), labels.to(self.device)\n",
    "            output = self(seqs)\n",
    "            predictions.extend(output.view(-1).detach().cpu().numpy())\n",
    "            actuals.extend(labels.view(-1).detach().cpu().numpy())\n",
    "\n",
    "        predictions_rescaled = list(self.label_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten())\n",
    "        actuals_rescaled = list(self.label_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten())\n",
    "        baseline_rescaled = [actuals_rescaled[0]] + actuals_rescaled[:-1]\n",
    "        baseline_constant = [0.0] * len(predictions_rescaled)\n",
    "\n",
    "        len_test_set = len(predictions)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(actuals_rescaled[-100:], label='Actual log returns', color='black', linestyle='-')\n",
    "        ax.plot(predictions_rescaled[-100:], label='Predicted log returns', color='green', linestyle='-')\n",
    "        # ax.plot(baseline_rescaled[-100:], label='Baseline_1', color='darkblue', linestyle='-')\n",
    "        # ax.plot(baseline_constant[-100:], label='Baseline_2', color='steelblue', linestyle='-')\n",
    "        # ax.plot(test_dates[-100:], arima_predictions[-100:], label='Baseline', color='orange', linestyle='-') \n",
    "        ax.set_title('Log returns prediction')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Log returns')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Relative Difference Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        actuals_rescaled = np.array(actuals_rescaled)\n",
    "        predictions_rescaled = np.array(predictions_rescaled)\n",
    "        baseline_rescaled = np.array(baseline_rescaled)\n",
    "\n",
    "        model_mse = mean_squared_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_rmse = np.sqrt(model_mse)\n",
    "        model_mae = mean_absolute_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_r2 = r2_score(actuals_rescaled, predictions_rescaled)\n",
    "        model_mape = np.mean(np.abs((actuals_rescaled - predictions_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label for label in actuals_rescaled]\n",
    "        pct_change_predictions = [pred for pred in predictions_rescaled]\n",
    "        hit_rate_model = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "        \n",
    "        baseline_mse = mean_squared_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_rmse = np.sqrt(baseline_mse)\n",
    "        baseline_mae = mean_absolute_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_r2 = r2_score(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_mape = np.mean(np.abs((actuals_rescaled - baseline_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "        pct_change_baseline = [base for base in baseline_rescaled]\n",
    "        hit_rate_baseline = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_baseline))\n",
    "\n",
    "        model_metrics = {\n",
    "            \"mse\": model_mse,\n",
    "            \"rmse\": model_rmse,\n",
    "            \"mae\": model_mae,\n",
    "            \"mape\": model_mape,\n",
    "            \"r2\": model_r2,\n",
    "            \"hit_rate\": hit_rate_model,\n",
    "        }\n",
    "        baseline_metrics = {\n",
    "            \"mse\": baseline_mse,\n",
    "            \"rmse\": baseline_rmse,\n",
    "            \"mae\": baseline_mae,\n",
    "            \"mape\": baseline_mape,\n",
    "            \"r2\": baseline_r2,\n",
    "            \"hit_rate\": hit_rate_baseline\n",
    "        }\n",
    "        model_baseline_performance_metrics = {\n",
    "            \"mse\": round((baseline_mse / model_mse - 1) * 100, 2),\n",
    "            \"rmse\": round((baseline_rmse / model_rmse - 1) * 100, 2),\n",
    "            \"mae\": round((baseline_mae / model_mae - 1) * 100, 2),\n",
    "            \"mape\": round((baseline_mape / model_mape - 1) * 100, 2),\n",
    "            \"r2\": round((model_r2 / baseline_r2 - 1) * 100, 2),\n",
    "            \"hit_rate\": round((hit_rate_model / hit_rate_baseline - 1) * 100, 2),\n",
    "        }\n",
    "\n",
    "        print(\"Preparing to log the table...\")\n",
    "        metrics_table = wandb.Table(columns=[\"metric\", \"model\", \"baseline\", \"model-baseline performance comparison [%]\"])\n",
    "        for metric in model_metrics.keys():\n",
    "            metrics_table.add_data(metric, model_metrics[metric], baseline_metrics[metric], model_baseline_performance_metrics[metric])\n",
    "        wandb.log({\"metrics\": metrics_table})\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    wandb_logger = WandbLogger(project=\"cyclical_data_forecast\", log_model=\"all\", config=wandb_config)\n",
    "    \n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    trainer = Trainer(max_epochs=num_epochs, logger=wandb_logger, accelerator=accelerator, devices=devices, enable_checkpointing=True)\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(dataloaders=test_loader, ckpt_path=\"best\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Optimization (Objective: Minimize validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:42:53,566] A new study created in memory with name: no-name-b84a5f77-ef01-4a2f-8049-b652ca88aa5f\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrederik135\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/Transformer/wandb/run-20240602_104254-6mirtshx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/6mirtshx' target=\"_blank\">good-cloud-1</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/6mirtshx' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/6mirtshx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 87.0 K\n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "87.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "87.0 K    Total params\n",
      "0.348     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22401b89d2094e02bd3acd1b004b07c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▁▂▂▃▃▃▁▂▃▂▂▃▃▂▂▄▄▄▃▃▄▅▆▅▅▆▇▇▆▆▇▇████▇██</td></tr><tr><td>train_loss_epoch</td><td>█▅▆▃▂▄▅▆▅▄▃▅▃▄▃▃▂▅▅▂▃▄▅▃▅▅▂▁▂▁▂▁▂▂▁▃▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▅▆▅▅▃▃▄▂▄▃▄▃▄▅▂▇▂▅▃▃▃▁▄▃▄▂▁▃▃▂▃▅▂▁▁▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▂▁▁</td></tr><tr><td>val_mape</td><td>▁▁▁▁▂▁▁▁▁▁▃▂▁▅▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇████▇██</td></tr><tr><td>val_rmse</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>hit_rate</td><td>0.67521</td></tr><tr><td>train_loss_epoch</td><td>0.00025</td></tr><tr><td>train_loss_step</td><td>0.01604</td></tr><tr><td>trainer/global_step</td><td>8750</td></tr><tr><td>val_loss</td><td>0.01979</td></tr><tr><td>val_mae</td><td>0.03979</td></tr><tr><td>val_mape</td><td>6.93717</td></tr><tr><td>val_mse</td><td>0.00339</td></tr><tr><td>val_r2</td><td>0.12755</td></tr><tr><td>val_rmse</td><td>0.05135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-cloud-1</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/6mirtshx' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/6mirtshx</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_104254-6mirtshx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:45:28,385] Trial 0 finished with value: 0.01979263871908188 and parameters: {'learning_rate': 2.3206582629665102e-05, 'num_layers': 2, 'num_heads': 2, 'hidden_size_multipliers': 5, 'dropout_prob': 0.2940441141780067}. Best is trial 0 with value: 0.01979263871908188.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81686749d3364db4b739f66a9f177003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011162767133334405, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/Transformer/wandb/run-20240602_104528-qswng91w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/qswng91w' target=\"_blank\">wild-sea-2</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/qswng91w' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/qswng91w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 386 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "386 K     Trainable params\n",
      "0         Non-trainable params\n",
      "386 K     Total params\n",
      "1.546     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7291f462e4464cca84928924b2fa0026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.013 MB uploaded\\r'), FloatProgress(value=0.08999343113641341, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▄▅▅▅▆▅▇▇▆▇▆▆▆▆▆▅▇█▅▅▆▆▆█▇▇█▆▆▇▇▆▇█▇▆█▇▆</td></tr><tr><td>train_loss_epoch</td><td>▂▃▆█▃▃▇▂▅▃▆▄▁█▄▅▄▅▃▄▅▃▃▂▄▁▄▃▃▁▃▄▁▃▄▅▃▄▄▂</td></tr><tr><td>train_loss_step</td><td>▂▇▆▅▂▆▃▃▅▄▆▇▆▂▅█▄▅▃▄▅▂▃▄▄▁▁▃▅▆▂▄▁▅▅▄▅▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▃▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▄▅▃▃▃▃▂▂▃▂▂▂▁▂▃▁▂▃▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_mape</td><td>▃▂▂▂▄▁▅▂▁▂▄▂▁▆▂▂▂▂▅█▆▁▂▄▂▂▄▃▅▂▁▁▁▄▂▁▂▄▁▁</td></tr><tr><td>val_mse</td><td>█▅▄▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▃▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁▅▆▄▇▇▇▇▇▇▆█▇███▇█▇▆▇█▇▇███▇███████▇████</td></tr><tr><td>val_rmse</td><td>█▄▄▅▃▃▃▂▂▂▃▂▂▂▂▂▂▁▂▃▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>69</td></tr><tr><td>hit_rate</td><td>0.68483</td></tr><tr><td>train_loss_epoch</td><td>0.00023</td></tr><tr><td>train_loss_step</td><td>0.02092</td></tr><tr><td>trainer/global_step</td><td>8625</td></tr><tr><td>val_loss</td><td>0.01836</td></tr><tr><td>val_mae</td><td>0.03779</td></tr><tr><td>val_mape</td><td>4.04315</td></tr><tr><td>val_mse</td><td>0.00314</td></tr><tr><td>val_r2</td><td>0.22206</td></tr><tr><td>val_rmse</td><td>0.04902</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sea-2</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/qswng91w' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/qswng91w</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_104528-qswng91w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:49:31,429] Trial 1 finished with value: 0.01836070790886879 and parameters: {'learning_rate': 2.1131427004628303e-05, 'num_layers': 3, 'num_heads': 2, 'hidden_size_multipliers': 15, 'dropout_prob': 0.04102967505433758}. Best is trial 1 with value: 0.01836070790886879.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58baab67e2a04710b85af7060e110a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011169037033331128, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/Transformer/wandb/run-20240602_104931-fkl7s8kk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/fkl7s8kk' target=\"_blank\">zany-salad-3</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/fkl7s8kk' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/fkl7s8kk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 55.6 K\n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "55.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.6 K    Total params\n",
      "0.222     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7e339e532549909f566974f5ed8c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>hit_rate</td><td>▆▃▁▂▅▅▄▅█▅▇▃▄▆▄▄▂▅▂▂</td></tr><tr><td>train_loss_epoch</td><td>▁▄▄▁▂▆▄▃▂▄▁▃▄▃▂▄█▂▆</td></tr><tr><td>train_loss_step</td><td>█▇▇▃▄▄▄▄▂▇▃▃▅▂▃▃▂▄▃▅▄▄▁▃▃▃▂▂▄▃▄▄▄▄▄▃▃▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_mape</td><td>▁▁▁▁▂▂▂▇▃▂▄▁▂▁▂▁█▁▁▁</td></tr><tr><td>val_mse</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁▅▆▇████████████████</td></tr><tr><td>val_rmse</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>hit_rate</td><td>0.53953</td></tr><tr><td>train_loss_epoch</td><td>0.00042</td></tr><tr><td>train_loss_step</td><td>0.01506</td></tr><tr><td>trainer/global_step</td><td>2375</td></tr><tr><td>val_loss</td><td>0.02422</td></tr><tr><td>val_mae</td><td>0.04489</td></tr><tr><td>val_mape</td><td>8.92362</td></tr><tr><td>val_mse</td><td>0.00414</td></tr><tr><td>val_r2</td><td>-0.10226</td></tr><tr><td>val_rmse</td><td>0.05723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-salad-3</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/fkl7s8kk' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/fkl7s8kk</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_104931-fkl7s8kk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:50:39,748] Trial 2 finished with value: 0.02421773597598076 and parameters: {'learning_rate': 2.0153782762766187e-05, 'num_layers': 3, 'num_heads': 2, 'hidden_size_multipliers': 2, 'dropout_prob': 0.32546031536770675}. Best is trial 1 with value: 0.01836070790886879.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf82f9deadd454cabf0c4cd640010d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011146500466667122, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/Transformer/wandb/run-20240602_105039-684ixbzj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/684ixbzj' target=\"_blank\">summer-leaf-4</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/684ixbzj' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/684ixbzj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 843 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "843 K     Trainable params\n",
      "0         Non-trainable params\n",
      "843 K     Total params\n",
      "3.375     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6b83a35c741b1aba0102cabb7d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▃▃▃▃▃▆▆▆▆███</td></tr><tr><td>hit_rate</td><td>▁▆▆█</td></tr><tr><td>train_loss_epoch</td><td>█▅▁</td></tr><tr><td>train_loss_step</td><td>██▃▃▂▁▂▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▃▄▄▄▄▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▁</td></tr><tr><td>val_mae</td><td>█▄▆▁</td></tr><tr><td>val_mape</td><td>▁█▂▃</td></tr><tr><td>val_mse</td><td>█▅▄▁</td></tr><tr><td>val_r2</td><td>▁▅▁█</td></tr><tr><td>val_rmse</td><td>█▅▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>hit_rate</td><td>0.66667</td></tr><tr><td>train_loss_epoch</td><td>0.00031</td></tr><tr><td>train_loss_step</td><td>0.02756</td></tr><tr><td>trainer/global_step</td><td>458</td></tr><tr><td>val_loss</td><td>0.02008</td></tr><tr><td>val_mae</td><td>0.03977</td></tr><tr><td>val_mape</td><td>9.85575</td></tr><tr><td>val_mse</td><td>0.00344</td></tr><tr><td>val_r2</td><td>0.14678</td></tr><tr><td>val_rmse</td><td>0.05135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-leaf-4</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/684ixbzj' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/684ixbzj</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_105039-684ixbzj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:51:07,267] Trial 3 finished with value: 0.020084626972675323 and parameters: {'learning_rate': 0.0033723082125739724, 'num_layers': 3, 'num_heads': 8, 'hidden_size_multipliers': 8, 'dropout_prob': 0.09329579969309237}. Best is trial 1 with value: 0.01836070790886879.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754f17e9094a419887456b365a34346e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011161284255556186, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/Transformer/wandb/run-20240602_105107-4gtzwz1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/4gtzwz1n' target=\"_blank\">spring-flower-5</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/4gtzwz1n' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/4gtzwz1n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 257 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "257 K     Trainable params\n",
      "0         Non-trainable params\n",
      "257 K     Total params\n",
      "1.029     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from models import TransformerModel\n",
    "from config import num_features, model_config, device, seq_length, architecture, wandb_config, num_epochs\n",
    "from cyclical_preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])\n",
    "    hidden_size_multipliers = trial.suggest_int('hidden_size_multipliers', 1, 16)\n",
    "    hidden_size = num_heads * hidden_size_multipliers\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    if hidden_size % num_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned(\"hidden_size is not divisible by num_heads\")\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": architecture,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": num_epochs\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"cyclical_hyperparameter_tuning_transformer\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = TransformerModel(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler,\n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"cyclical_hyperparameter_tuning_transformer\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
