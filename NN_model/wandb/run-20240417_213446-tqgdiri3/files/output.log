
Epoch 0: 100%|██████████| 48/48 [00:00<00:00, 108.10it/s, v_num=iri3, train_loss_step=0.178, val_loss_step=0.0667, val_loss_epoch=0.0687, train_loss_epoch=0.0686]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name      | Type     | Params
---------------------------------------
0 | model     | RNNModel | 2.3 K
1 | criterion | MSELoss  | 0
---------------------------------------
2.3 K     Trainable params
0         Non-trainable params
2.3 K     Total params
0.009     Total estimated model params size (MB)
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\torch\nn\modules\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\pytorch_lightning\loops\fit_loop.py:298: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\torch\nn\modules\loss.py:535: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\janfr\anaconda3\envs\pytorch_3_10\lib\site-packages\torch\nn\modules\loss.py:535: UserWarning: Using a target size (torch.Size([44])) that is different to the input size (torch.Size([44, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.



























Epoch 29: 100%|██████████| 48/48 [00:00<00:00, 64.57it/s, v_num=iri3, train_loss_step=0.170, val_loss_step=0.0642, val_loss_epoch=0.0691, train_loss_epoch=0.0649]
Testing DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 315.05it/s]364
364
363
0
`Trainer.fit` stopped: `max_epochs=30` reached.
Restoring states from the checkpoint path at .\RNN_single_step_forecasts\tqgdiri3\checkpoints\epoch=29-step=1440.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at .\RNN_single_step_forecasts\tqgdiri3\checkpoints\epoch=29-step=1440.ckpt
