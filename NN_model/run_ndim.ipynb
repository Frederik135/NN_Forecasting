{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from config import num_epochs, learning_rate, wandb_config, model\n",
    "from preprocessing import stock_df\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler, test_dates, auto_arima_model\n",
    "\n",
    "class StockPredictionModule(pl.LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader, test_dates):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.test_dates = test_dates\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # self.criterion = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        predictions, actuals = [], []\n",
    "        for seqs, labels in self.test_loader:\n",
    "            seqs, labels = seqs.to(self.device), labels.to(self.device)\n",
    "            output = self(seqs)\n",
    "            predictions.extend(output.view(-1).detach().cpu().numpy())\n",
    "            actuals.extend(labels.view(-1).detach().cpu().numpy())\n",
    "\n",
    "        predictions_rescaled = list(self.label_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten())\n",
    "        actuals_rescaled = list(self.label_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten())\n",
    "        baseline_rescaled = [actuals_rescaled[0]] + actuals_rescaled[:-1]\n",
    "        arima_predictions = auto_arima_model.predict(n_periods=len(actuals_rescaled))\n",
    "        baseline_constant = [1.0] * len(predictions_rescaled)\n",
    "\n",
    "        len_test_set = len(predictions)\n",
    "        actual_closing_prices = stock_df['Close'].values[-(len_test_set):]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(test_dates[-100:], actuals_rescaled[-100:], label='Actual Relative Difference', color='black', linestyle='-')\n",
    "        ax.plot(test_dates[-100:], predictions_rescaled[-100:], label='Predicted Relative Difference', color='green', linestyle='-')\n",
    "        ax.plot(test_dates[-100:], baseline_rescaled[-100:], label='Baseline_1', color='darkblue', linestyle='-')\n",
    "        ax.plot(test_dates[-100:], baseline_constant[-100:], label='Baseline_2', color='steelblue', linestyle='-')\n",
    "        # ax.plot(test_dates[-100:], arima_predictions[-100:], label='Baseline', color='orange', linestyle='-') \n",
    "        ax.set_title('Relative Difference Prediction')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Relative Difference')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Relative Difference Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Use plotting with rebasing to visualize the predictions as prices\n",
    "        rebase_period = 30\n",
    "        predicted_prices = [actual_closing_prices[0]]\n",
    "        for i, relative_change in enumerate(predictions_rescaled[1:], 1):\n",
    "            if i % rebase_period == 0:\n",
    "                predicted_prices.append(actual_closing_prices[i])\n",
    "            else:\n",
    "                predicted_prices.append(predicted_prices[-1] * relative_change)\n",
    "        baseline_prices = [actual_closing_prices[0]] + list(actual_closing_prices[:-1])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(test_dates, actual_closing_prices, label='Actual Price', color='black', linestyle='-')\n",
    "        ax.plot(test_dates, predicted_prices, label='Predicted Price', color='green', linestyle='-')\n",
    "        ax.plot(test_dates, baseline_prices, label='Baseline', color='blue', linestyle='-') \n",
    "        ax.set_title('Price predictions based on last price in validation set')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Stock Price Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # We use for the first value the actual closing price and then multiply the relative change for each following timestep\n",
    "        actual_prices = [actual_closing_prices[0]]\n",
    "        for i in range(1, len(actuals_rescaled)):\n",
    "            actual_prices.append(actual_prices[i-1] * actuals_rescaled[i])\n",
    "        prediction_prices = [actual_closing_prices[0]]\n",
    "        for i in range(1, len(predictions_rescaled)):\n",
    "            prediction_prices.append(prediction_prices[i-1] * predictions_rescaled[i])\n",
    "        baseline_prices = [actual_prices[0]] + actual_prices[:-1]\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(test_dates, actual_prices, label='Actual Price', color='black', linestyle='-')\n",
    "        ax.plot(test_dates, prediction_prices, label='Predicted Price', color='green', linestyle='-')\n",
    "        ax.plot(test_dates, baseline_prices, label='Baseline', color='blue', linestyle='-')\n",
    "        ax.set_title('Stock Price Prediction')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Stock Price Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \"\"\"\n",
    "        \n",
    "        net_abs_dev = torch.tensor([abs(predictions_rescaled[i] - actuals_rescaled[i]) for i in range(len(actuals_rescaled))])\n",
    "        baseline_abs_dev = torch.tensor([abs(baseline_rescaled[i] - actuals_rescaled[i]) for i in range(len(actuals_rescaled))])\n",
    "        diff_pos = torch.relu(baseline_abs_dev - net_abs_dev).reshape(-1).tolist()\n",
    "        diff_min = (-torch.relu(net_abs_dev - baseline_abs_dev)).reshape(-1).tolist()\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Model vs baseline performance comparison on test samples')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), diff_pos, color='g', label='Model Wins', width=1.0)\n",
    "        ax.bar(list(range(len(actuals_rescaled))), diff_min, color='r', label='Baseline Wins', width=1.0)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Difference in Absolute Deviation')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Model vs Baseline Performance Comparison\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        model_actual_dev = torch.tensor([predictions_rescaled[i] - actuals_rescaled[i] for i in range(len(actuals_rescaled))])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Model deviations from actuals')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), model_actual_dev, color='g', label='Model Wins', width=1.0)\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Deviation from actuals')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Model deviations from actuals\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        baseline_actual_dev = torch.tensor([baseline_rescaled[i] - actuals_rescaled[i] for i in range(len(actuals_rescaled))])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Baseline deviations from actuals')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), baseline_actual_dev, color='b', label='Baseline Wins', width=1.0)\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Deviation from actuals')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Baseline deviations from actuals\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        arima_actual_dev = torch.tensor([arima_predictions[i] - actuals_rescaled[i] for i in range(len(actuals_rescaled))])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Arima deviations from actuals')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), arima_actual_dev, color='orange', width=1.0)\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Deviation from actuals')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Arima deviations from actuals\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        actuals_rescaled = np.array(actuals_rescaled)\n",
    "        predictions_rescaled = np.array(predictions_rescaled)\n",
    "        baseline_rescaled = np.array(baseline_rescaled)\n",
    "        arima_predictions = np.array(arima_predictions)\n",
    "\n",
    "        model_mse = mean_squared_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_rmse = np.sqrt(model_mse)\n",
    "        model_mae = mean_absolute_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_r2 = r2_score(actuals_rescaled, predictions_rescaled)\n",
    "        model_mape = np.mean(np.abs((actuals_rescaled - predictions_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "        \n",
    "        baseline_mse = mean_squared_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_rmse = np.sqrt(baseline_mse)\n",
    "        baseline_mae = mean_absolute_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_r2 = r2_score(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_mape = np.mean(np.abs((actuals_rescaled - baseline_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "\n",
    "        arima_mse = mean_squared_error(actuals_rescaled, arima_predictions)\n",
    "        arima_rmse = np.sqrt(arima_mse)\n",
    "        arima_mae = mean_absolute_error(actuals_rescaled, arima_predictions)\n",
    "        arima_r2 = r2_score(actuals_rescaled, arima_predictions)\n",
    "        arima_mape = np.mean(np.abs((actuals_rescaled - arima_predictions) / actuals_rescaled))\n",
    "\n",
    "        model_metrics = {\n",
    "            \"mse\": model_mse,\n",
    "            \"rmse\": model_rmse,\n",
    "            \"mae\": model_mae,\n",
    "            \"mape\": model_mape,\n",
    "            \"r2\": model_r2,\n",
    "        }\n",
    "        baseline_metrics = {\n",
    "            \"mse\": baseline_mse,\n",
    "            \"rmse\": baseline_rmse,\n",
    "            \"mae\": baseline_mae,\n",
    "            \"mape\": baseline_mape,\n",
    "            \"r2\": baseline_r2,\n",
    "        }\n",
    "        arima_metrics = {\n",
    "            \"mse\": arima_mse,\n",
    "            \"rmse\": arima_rmse,\n",
    "            \"mae\": arima_mae,\n",
    "            \"mape\": arima_mape,\n",
    "            \"r2\": arima_r2,\n",
    "        }\n",
    "        model_baseline_performance_metrics = {\n",
    "            \"mse\": round((baseline_mse / model_mse - 1) * 100, 2),\n",
    "            \"rmse\": round((baseline_rmse / model_rmse - 1) * 100, 2),\n",
    "            \"mae\": round((baseline_mae / model_mae - 1) * 100, 2),\n",
    "            \"mape\": round((baseline_mape / model_mape - 1) * 100, 2),\n",
    "            \"r2\": round((model_r2 / baseline_r2 - 1) * 100, 2),\n",
    "        }\n",
    "        model_arima_performance_metrics = {\n",
    "            \"mse\": round((arima_mse / model_mse - 1) * 100, 2),\n",
    "            \"rmse\": round((arima_rmse / model_rmse - 1) * 100, 2),\n",
    "            \"mae\": round((arima_mae / model_mae - 1) * 100, 2),\n",
    "            \"mape\": round((arima_mape / model_mape - 1) * 100, 2),\n",
    "            \"r2\": round((model_r2 / arima_r2 - 1) * 100, 2),\n",
    "        }\n",
    "\n",
    "        metrics_table = wandb.Table(columns=[\"metric\", \"model\", \"baseline\", \"arima\", \"model-baseline performance comparison [%]\", \n",
    "                                             \"model-arima performance comparison [%]\"])\n",
    "        for metric in model_metrics.keys():\n",
    "            metrics_table.add_data(metric, model_metrics[metric], baseline_metrics[metric], arima_metrics[metric], \n",
    "                                   model_baseline_performance_metrics[metric], model_arima_performance_metrics[metric])\n",
    "        wandb.log({\"metrics\": metrics_table})\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    wandb_logger = WandbLogger(project=\"RNN_single_step_forecasts\", log_model=\"all\", config=wandb_config)\n",
    "    \n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, test_dates = test_dates)\n",
    "\n",
    "    # Device agnostic initialization\n",
    "    if torch.cuda.is_available():   # Check for GPU availability\n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif hasattr(torch, 'has_mps') and torch.backends.mps.is_built():  # Check for MPS availability (Apple Silicon)\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None  # Defaults to CPU\n",
    "        devices = None  # Ignored for CPU\n",
    "\n",
    "    trainer = Trainer(max_epochs=num_epochs, logger=wandb_logger, accelerator=accelerator, devices=devices, enable_checkpointing=True)\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(dataloaders=test_loader, ckpt_path=\"best\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust optuna code below with these changes to log performance metrics:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    # Existing methods remain unchanged\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return {\"y_pred\": y_pred, \"labels\": labels}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_result = self.trainer.test(self, dataloaders=self.test_loader, verbose=False)\n",
    "        predictions = np.concatenate([x['y_pred'].cpu().numpy() for x in test_result])\n",
    "        actuals = np.concatenate([x['labels'].cpu().numpy() for x in test_result])\n",
    "\n",
    "        # Assuming predictions and actuals are properly scaled or normalized if necessary\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "\n",
    "        self.log_dict({\n",
    "            \"model_mse\": mse,\n",
    "            \"model_rmse\": rmse,\n",
    "            \"model_r2\": r2\n",
    "        })\n",
    "\n",
    "def objective(trial):\n",
    "    # Configuration setup remains the same\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", reinit=True, config=wandb_config)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler,\n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, test_dates=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    # Trainer and training setup remains the same\n",
    "    \n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(module, dataloaders=test_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "\n",
    "    # Fetch the logged metrics for return to Optuna\n",
    "    mse = module.logged_metrics.get('model_mse', float('inf'))\n",
    "    rmse = module.logged_metrics.get('model_rmse', float('inf'))\n",
    "    r2 = module.logged_metrics.get('model_r2', float('inf'))\n",
    "\n",
    "    # Finish the W&B run and return a metric for Optuna to minimize\n",
    "    wandb.log({\"val_loss\": val_loss, \"mse\": mse, \"rmse\": rmse, \"r2\": r2})\n",
    "    wandb.finish()\n",
    "\n",
    "    return val_loss  # Or return another metric such as mse if that's your focus\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[I 2024-05-02 20:41:38,416] A new study created in memory with name: no-name-7b8c4611-ae36-46eb-aa83-041a2719691e\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrederik135\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204140-96cl0tz7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/96cl0tz7' target=\"_blank\">sunny-dust-210</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/96cl0tz7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/96cl0tz7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 4.8 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "4.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 K     Total params\n",
      "0.019     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▇▂▃▃▁▃▁▆▁▃▁▃▃▃▄▁▄▃▃▁▃▂▂▆▁█▂▅▃▄▁▄▃▆▅▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▅▃▄▃▂▂▂▁▅▁▁█▂▃▂▃▁▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00244</td></tr><tr><td>trainer/global_step</td><td>2502</td></tr><tr><td>val_loss</td><td>0.00171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-dust-210</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/96cl0tz7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/96cl0tz7</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204140-96cl0tz7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:42:23,216] Trial 0 finished with value: 0.00170747225638479 and parameters: {'learning_rate': 0.021283024611366413, 'num_layers': 2, 'hidden_size': 20, 'dropout_prob': 0.09451885489639611}. Best is trial 0 with value: 0.00170747225638479.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204223-6ftumwkx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6ftumwkx' target=\"_blank\">smart-yogurt-211</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6ftumwkx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6ftumwkx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 39.3 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "39.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.3 K    Total params\n",
      "0.157     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▆▂▃▃▃▁▄█▁▅▁▄▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▂██▂▅▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▇▂▃▃▁▃▂▃▃▃▃▅█▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00092</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-yogurt-211</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6ftumwkx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6ftumwkx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204223-6ftumwkx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:43:00,538] Trial 1 finished with value: 0.001668485114350915 and parameters: {'learning_rate': 0.0015123678786440474, 'num_layers': 3, 'hidden_size': 49, 'dropout_prob': 0.3578247104322038}. Best is trial 1 with value: 0.001668485114350915.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204300-ija690on</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ija690on' target=\"_blank\">bumbling-leaf-212</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ija690on' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ija690on</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.06678366469905106 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 55.4 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "55.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.4 K    Total params\n",
      "0.221     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▇▂▃▂▃▁▄█▁▆▁▄▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▁▃▃▂▃▃▄▅▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss_epoch</td><td>0.00486</td></tr><tr><td>train_loss_step</td><td>0.00208</td></tr><tr><td>trainer/global_step</td><td>1529</td></tr><tr><td>val_loss</td><td>0.00171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-leaf-212</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ija690on' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ija690on</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204300-ija690on\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:43:48,155] Trial 2 finished with value: 0.0017113479552790523 and parameters: {'learning_rate': 0.0008372464986001755, 'num_layers': 1, 'hidden_size': 127, 'dropout_prob': 0.06678366469905106}. Best is trial 1 with value: 0.001668485114350915.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204348-tgu27vsz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tgu27vsz' target=\"_blank\">honest-haze-213</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tgu27vsz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tgu27vsz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 113 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "113 K     Trainable params\n",
      "0         Non-trainable params\n",
      "113 K     Total params\n",
      "0.454     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▂▂▂▁▂▂▁▂▂▂▁▁▂▁▁▁▃▃▁▂▂▁▂▃▂▁▁▂▁▁▁▂█▁▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▃▁▃▄▃▂▁▁▁▂▂▂▂▁▂▁▆▂██▃▄▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24</td></tr><tr><td>train_loss_epoch</td><td>0.00449</td></tr><tr><td>train_loss_step</td><td>0.00093</td></tr><tr><td>trainer/global_step</td><td>3336</td></tr><tr><td>val_loss</td><td>0.0017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-haze-213</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tgu27vsz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tgu27vsz</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204348-tgu27vsz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:44:34,393] Trial 3 finished with value: 0.0017043034313246608 and parameters: {'learning_rate': 0.0008761997405918035, 'num_layers': 2, 'hidden_size': 109, 'dropout_prob': 0.01053599849113207}. Best is trial 1 with value: 0.001668485114350915.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204434-1j5bzlms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1j5bzlms' target=\"_blank\">lilac-cosmos-214</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1j5bzlms' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1j5bzlms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.42133082359134855 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 8.6 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▂▃▂▁▄█▁▅▁▂▁▃▃▃▄▁▄▂▂▃▃▂▂▂▅█▇▂▅▂▄▁▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▃▅▅▃▂▁▁▂▃▅▇▃▁▄█▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00466</td></tr><tr><td>train_loss_step</td><td>0.00195</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-cosmos-214</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1j5bzlms' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1j5bzlms</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204434-1j5bzlms\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:45:14,499] Trial 4 finished with value: 0.001694772974587977 and parameters: {'learning_rate': 0.00659011418041559, 'num_layers': 1, 'hidden_size': 45, 'dropout_prob': 0.42133082359134855}. Best is trial 1 with value: 0.001668485114350915.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204514-7dkbhp8h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7dkbhp8h' target=\"_blank\">hearty-feather-215</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7dkbhp8h' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7dkbhp8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 108 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "108 K     Trainable params\n",
      "0         Non-trainable params\n",
      "108 K     Total params\n",
      "0.436     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▁▁▁▁▁▂▂▁▁▁▂▁▂▁▂▂▁▂▁▁█▁▂▂▅▂▂▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00193</td></tr><tr><td>trainer/global_step</td><td>5004</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-feather-215</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7dkbhp8h' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7dkbhp8h</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204514-7dkbhp8h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:46:06,664] Trial 5 finished with value: 0.0016529522836208344 and parameters: {'learning_rate': 1.4973472487106058e-05, 'num_layers': 3, 'hidden_size': 83, 'dropout_prob': 0.40016148651222216}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204606-qxifrmwq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qxifrmwq' target=\"_blank\">dry-sound-216</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qxifrmwq' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qxifrmwq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 71.8 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "71.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "71.8 K    Total params\n",
      "0.287     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▆▂▃▃▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▅▁▃▃▄▃▃▂▇▇▃██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss_epoch</td><td>0.00488</td></tr><tr><td>train_loss_step</td><td>0.00148</td></tr><tr><td>trainer/global_step</td><td>1668</td></tr><tr><td>val_loss</td><td>0.0017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sound-216</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qxifrmwq' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qxifrmwq</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204606-qxifrmwq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:46:44,928] Trial 6 finished with value: 0.0016984553076326847 and parameters: {'learning_rate': 0.013951908686201143, 'num_layers': 2, 'hidden_size': 86, 'dropout_prob': 0.17853774901136743}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204644-spnbxnx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/spnbxnx4' target=\"_blank\">royal-frost-217</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/spnbxnx4' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/spnbxnx4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 178 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "178 K     Trainable params\n",
      "0         Non-trainable params\n",
      "178 K     Total params\n",
      "0.716     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▄▂▂▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▃</td></tr><tr><td>train_loss_step</td><td>▂▁▁▂▂▂▁▁▂▁▂▃▂▂▁▁▂▁▃▄▃▃▂▁▂▃▃▂▂▂▂▁▁▂▄▃▂▂▂█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.02898</td></tr><tr><td>trainer/global_step</td><td>3475</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-frost-217</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/spnbxnx4' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/spnbxnx4</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204644-spnbxnx4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:47:33,501] Trial 7 finished with value: 0.0016578750219196081 and parameters: {'learning_rate': 1.705345249558513e-05, 'num_layers': 3, 'hidden_size': 107, 'dropout_prob': 0.06489591942745587}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204733-t7d1ctde</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t7d1ctde' target=\"_blank\">glowing-donkey-218</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t7d1ctde' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t7d1ctde</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 24.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "24.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 K    Total params\n",
      "0.098     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▆▂▃▃▁▃█▁▅▁▂▁▃▃▃▄▁▄▃▂▃▃▂▂▂▅██▂▅▂▄▁▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▃▃▃▂▁▁▂▁▂▂▃▂▃▆█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00475</td></tr><tr><td>train_loss_step</td><td>0.00207</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-donkey-218</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t7d1ctde' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t7d1ctde</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204733-t7d1ctde\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:48:14,679] Trial 8 finished with value: 0.0017050658352673054 and parameters: {'learning_rate': 0.004935327370341839, 'num_layers': 2, 'hidden_size': 49, 'dropout_prob': 0.3378624273022254}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204814-yc2w1buc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yc2w1buc' target=\"_blank\">restful-dragon-219</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yc2w1buc' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yc2w1buc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.32545138983188676 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 17.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "17.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.6 K    Total params\n",
      "0.070     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▃▂▂▁▂▂▂▂▁▁▁▁▁▃▂▁▁▂▂▂▂▁▁▁▁▂█▁▂▂▂▇▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▂▃▂▃▃▄▅▃▂▄▄▄▅▂▅▁▄▃▅█▄▁▃▅▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>0.00479</td></tr><tr><td>train_loss_step</td><td>0.001</td></tr><tr><td>trainer/global_step</td><td>3753</td></tr><tr><td>val_loss</td><td>0.00169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">restful-dragon-219</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yc2w1buc' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yc2w1buc</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204814-yc2w1buc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:49:00,986] Trial 9 finished with value: 0.0016931580612435937 and parameters: {'learning_rate': 0.010560011694775887, 'num_layers': 1, 'hidden_size': 68, 'dropout_prob': 0.32545138983188676}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204901-v12yjypg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v12yjypg' target=\"_blank\">eager-breeze-220</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v12yjypg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v12yjypg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 108 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "108 K     Trainable params\n",
      "0         Non-trainable params\n",
      "108 K     Total params\n",
      "0.436     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▃▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▃▃▃▃▂▃█▅▁▃▂▁▃▃▃▆▄▄▃▂▃▁▃▂▂▂▆▇▇▂▅▂▃▂▁▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▆▃▂▁▃▂▄▄▃▄▃▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>train_loss_epoch</td><td>0.00486</td></tr><tr><td>train_loss_step</td><td>0.00862</td></tr><tr><td>trainer/global_step</td><td>2224</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-breeze-220</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v12yjypg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v12yjypg</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204901-v12yjypg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:49:41,392] Trial 10 finished with value: 0.0016531620640307665 and parameters: {'learning_rate': 2.1569036232551318e-05, 'num_layers': 3, 'hidden_size': 83, 'dropout_prob': 0.48198458527736254}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_204941-8pfdmfc1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8pfdmfc1' target=\"_blank\">daily-deluge-221</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8pfdmfc1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8pfdmfc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 89.4 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "89.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "89.4 K    Total params\n",
      "0.358     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃▂▂▃▂▂▂▂▂▁▃▄▁▁▃▃▁▁▁▁▂▂▂█▂▁▂▁▂▂▂▂▂▂▁▂▂▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00909</td></tr><tr><td>trainer/global_step</td><td>5699</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-deluge-221</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8pfdmfc1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8pfdmfc1</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_204941-8pfdmfc1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:50:37,737] Trial 11 finished with value: 0.0016563143581151962 and parameters: {'learning_rate': 1.3280516532163572e-05, 'num_layers': 3, 'hidden_size': 75, 'dropout_prob': 0.4695054627847605}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205037-9gdjw2vr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9gdjw2vr' target=\"_blank\">royal-fire-222</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9gdjw2vr' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9gdjw2vr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 135 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "135 K     Trainable params\n",
      "0         Non-trainable params\n",
      "135 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▂█▁▂▁▁▂▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▆▄▄▃▂▂▁▁▁▁▂▂▂▁▂▁▁▂▁▁▂▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00229</td></tr><tr><td>trainer/global_step</td><td>4170</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-fire-222</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9gdjw2vr' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9gdjw2vr</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205037-9gdjw2vr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:51:28,088] Trial 12 finished with value: 0.0016536619514226913 and parameters: {'learning_rate': 7.877688956214829e-05, 'num_layers': 3, 'hidden_size': 93, 'dropout_prob': 0.4960438603992068}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205128-97u20uqz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/97u20uqz' target=\"_blank\">avid-wave-223</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/97u20uqz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/97u20uqz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 76.0 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "76.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "76.0 K    Total params\n",
      "0.304     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▂▂▂▁▁▂▁▁▂█▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00516</td></tr><tr><td>trainer/global_step</td><td>3614</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-wave-223</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/97u20uqz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/97u20uqz</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205128-97u20uqz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:52:16,328] Trial 13 finished with value: 0.001654800376854837 and parameters: {'learning_rate': 9.349311573279189e-05, 'num_layers': 3, 'hidden_size': 69, 'dropout_prob': 0.25671074487200707}. Best is trial 5 with value: 0.0016529522836208344.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205216-9t7ds4jn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9t7ds4jn' target=\"_blank\">resilient-planet-224</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9t7ds4jn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9t7ds4jn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 141 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▃▃▃▄▁▄█▂▅▂▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▂██▂▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▂▂▂▂▂▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>14</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00258</td></tr><tr><td>trainer/global_step</td><td>1946</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-planet-224</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9t7ds4jn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9t7ds4jn</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205216-9t7ds4jn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:52:54,290] Trial 14 finished with value: 0.001652318867854774 and parameters: {'learning_rate': 8.115786597563777e-05, 'num_layers': 3, 'hidden_size': 95, 'dropout_prob': 0.4182874245778097}. Best is trial 14 with value: 0.001652318867854774.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205254-ma2d2qub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ma2d2qub' target=\"_blank\">iconic-universe-225</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ma2d2qub' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ma2d2qub</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 162 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "162 K     Trainable params\n",
      "0         Non-trainable params\n",
      "162 K     Total params\n",
      "0.652     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▄▁▅▁▂▂▆▂▃▅▂▂▂▆▅▁▅▃▃▄▂▃▃█▂▂▂▇▃▂▁▄▂▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▄▃▃▃▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▃▂▂▂▃▂▁▃▂▁▂▄▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.0036</td></tr><tr><td>trainer/global_step</td><td>7228</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-universe-225</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ma2d2qub' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ma2d2qub</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205254-ma2d2qub\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:53:58,846] Trial 15 finished with value: 0.0016582064563408494 and parameters: {'learning_rate': 0.00014242162512679654, 'num_layers': 3, 'hidden_size': 102, 'dropout_prob': 0.39914110158859956}. Best is trial 14 with value: 0.001652318867854774.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205358-qohooy3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qohooy3h' target=\"_blank\">playful-pond-226</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qohooy3h' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qohooy3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 145 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "145 K     Trainable params\n",
      "0         Non-trainable params\n",
      "145 K     Total params\n",
      "0.584     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▁▁▁▁▁▂▂▁▁▁▂▁▂▁▂▂▁▂▁▁█▁▂▂▅▂▂▁▁▂▁▁▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆█▆▅▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▂▂▂▃▃▄▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.00473</td></tr><tr><td>train_loss_step</td><td>0.00178</td></tr><tr><td>trainer/global_step</td><td>5004</td></tr><tr><td>val_loss</td><td>0.00167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-pond-226</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qohooy3h' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qohooy3h</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205358-qohooy3h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:54:51,163] Trial 16 finished with value: 0.0016651630867272615 and parameters: {'learning_rate': 0.0002522503055437818, 'num_layers': 2, 'hidden_size': 124, 'dropout_prob': 0.250576334005562}. Best is trial 14 with value: 0.001652318867854774.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205451-8ent5slx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8ent5slx' target=\"_blank\">dainty-capybara-227</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8ent5slx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8ent5slx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 156 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "156 K     Trainable params\n",
      "0         Non-trainable params\n",
      "156 K     Total params\n",
      "0.627     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▂▃▃▃▁▄█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁█▇▂▅▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▄▃▅▇▆▅▅▆▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00095</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-capybara-227</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8ent5slx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8ent5slx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205451-8ent5slx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:55:31,549] Trial 17 finished with value: 0.001650989055633545 and parameters: {'learning_rate': 4.0419738205451154e-05, 'num_layers': 3, 'hidden_size': 100, 'dropout_prob': 0.41140814749236226}. Best is trial 17 with value: 0.001650989055633545.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205531-9h9d94x6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9h9d94x6' target=\"_blank\">fearless-meadow-228</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9h9d94x6' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9h9d94x6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 206 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.824     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▆▂▃▃▁▃█▁▅▁▂▁▃▃▃▄▁▄▂▂▃▃▂▂▂▆██▂▅▂▄▁▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▁▁▁▁▁▁▁▂▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00206</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-meadow-228</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9h9d94x6' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9h9d94x6</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205531-9h9d94x6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:56:11,814] Trial 18 finished with value: 0.0016547052655369043 and parameters: {'learning_rate': 4.447355679787634e-05, 'num_layers': 3, 'hidden_size': 115, 'dropout_prob': 0.2993096717250861}. Best is trial 17 with value: 0.001650989055633545.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205611-hvmesu88</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/hvmesu88' target=\"_blank\">floral-river-229</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/hvmesu88' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/hvmesu88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 88.8 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "88.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "88.8 K    Total params\n",
      "0.355     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▁▂▂▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁█▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▃▅█▆▄▄▅▃▃▃▂▂▁▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.0063</td></tr><tr><td>trainer/global_step</td><td>3197</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-river-229</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/hvmesu88' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/hvmesu88</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205611-hvmesu88\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:56:57,736] Trial 19 finished with value: 0.001660130568780005 and parameters: {'learning_rate': 0.0003030580689631156, 'num_layers': 2, 'hidden_size': 96, 'dropout_prob': 0.19323649509308824}. Best is trial 17 with value: 0.001650989055633545.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205657-9yx5lesx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9yx5lesx' target=\"_blank\">serene-sunset-230</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9yx5lesx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9yx5lesx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 59.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "59.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "59.9 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▂▂▂▁▂▁▆▂█▂▂▁▁▂▂▁▁▁▁▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>train_loss_epoch</td><td>0.00535</td></tr><tr><td>train_loss_step</td><td>0.0017</td></tr><tr><td>trainer/global_step</td><td>4309</td></tr><tr><td>val_loss</td><td>0.00184</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-sunset-230</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9yx5lesx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/9yx5lesx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205657-9yx5lesx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:57:50,077] Trial 20 finished with value: 0.0018416685052216053 and parameters: {'learning_rate': 0.05760997784488788, 'num_layers': 3, 'hidden_size': 61, 'dropout_prob': 0.4502901657802649}. Best is trial 17 with value: 0.001650989055633545.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205750-yhlekc6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yhlekc6s' target=\"_blank\">blooming-morning-231</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yhlekc6s' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yhlekc6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 114 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "114 K     Trainable params\n",
      "0         Non-trainable params\n",
      "114 K     Total params\n",
      "0.456     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▂▂▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▆▂▃▂▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▂▂▃▁▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▁▃▃▄▇█▆█▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss_epoch</td><td>0.00488</td></tr><tr><td>train_loss_step</td><td>0.00201</td></tr><tr><td>trainer/global_step</td><td>1529</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-morning-231</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yhlekc6s' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yhlekc6s</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205750-yhlekc6s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:58:28,258] Trial 21 finished with value: 0.001650880090892315 and parameters: {'learning_rate': 3.6406364933088336e-05, 'num_layers': 3, 'hidden_size': 85, 'dropout_prob': 0.3791988901314319}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205828-2ql5ciy1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2ql5ciy1' target=\"_blank\">ancient-brook-232</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2ql5ciy1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2ql5ciy1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 135 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "135 K     Trainable params\n",
      "0         Non-trainable params\n",
      "135 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▂▃▄▂█▁▅▃▂▁▃▃▆▄▄▂▂▁▃▂▂▇▂█▂▅▂▃▁▄▆▅▅▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00261</td></tr><tr><td>trainer/global_step</td><td>2641</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ancient-brook-232</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2ql5ciy1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2ql5ciy1</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205828-2ql5ciy1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:59:10,047] Trial 22 finished with value: 0.0016529182903468609 and parameters: {'learning_rate': 4.166931107664795e-05, 'num_layers': 3, 'hidden_size': 93, 'dropout_prob': 0.3804878918496578}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205910-xoctob7c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xoctob7c' target=\"_blank\">chocolate-bee-233</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xoctob7c' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xoctob7c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 216 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "216 K     Trainable params\n",
      "0         Non-trainable params\n",
      "216 K     Total params\n",
      "0.867     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▂▁▃▂▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▁▁▂▁▂▁▁▁▂▁▁▁▁█▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▂▁▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00628</td></tr><tr><td>trainer/global_step</td><td>3197</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-bee-233</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xoctob7c' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xoctob7c</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205910-xoctob7c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 20:59:54,240] Trial 23 finished with value: 0.0016532085137441754 and parameters: {'learning_rate': 4.145070195141257e-05, 'num_layers': 3, 'hidden_size': 118, 'dropout_prob': 0.43588999412637974}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_205954-u06yum03</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/u06yum03' target=\"_blank\">vocal-feather-234</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/u06yum03' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/u06yum03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 92.4 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "92.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "92.4 K    Total params\n",
      "0.370     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▆▃▂▅▂▂▁▆▂▂▅▄▂▂▁▆▃▁▅▃▂▁▂▁▃▂█▁▁▂▆▃▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▆███▇█▅▅▅▄▄▃▂▃▃▄▄▂▂▂▃▃▃▄▃▃▃▁▃▂▂▅▄▄▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.00463</td></tr><tr><td>train_loss_step</td><td>0.00113</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.00169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-feather-234</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/u06yum03' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/u06yum03</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_205954-u06yum03\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:00:54,832] Trial 24 finished with value: 0.0016926145181059837 and parameters: {'learning_rate': 0.0003123043184715622, 'num_layers': 2, 'hidden_size': 98, 'dropout_prob': 0.2925147995891152}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210054-6kxo2zcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6kxo2zcn' target=\"_blank\">amber-disco-235</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6kxo2zcn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6kxo2zcn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 96.5 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "96.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "96.5 K    Total params\n",
      "0.386     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▄▃▁▄▁▆▄▂▃▃▆▄▁▃▂▁▃▂▂▂█▂▅▃▂▁▃▆▅▂▃▂▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▁▂▂▂▁▁▂▂▁▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>21</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00299</td></tr><tr><td>trainer/global_step</td><td>2919</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-disco-235</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6kxo2zcn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6kxo2zcn</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210054-6kxo2zcn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:01:38,752] Trial 25 finished with value: 0.0016556879272684455 and parameters: {'learning_rate': 0.00010474651933325192, 'num_layers': 3, 'hidden_size': 78, 'dropout_prob': 0.37345618325583635}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210138-wpb96g8i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wpb96g8i' target=\"_blank\">apricot-firefly-236</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wpb96g8i' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wpb96g8i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 59.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "59.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "59.9 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▂▄▁▅▂▂▅▂▂▃▄▂▂▂▅▁▅▃▂▁▂▂▂█▁▂▁▃▃▁▁▂▁▃▅▅▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>54</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.0013</td></tr><tr><td>trainer/global_step</td><td>7506</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-firefly-236</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wpb96g8i' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wpb96g8i</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210138-wpb96g8i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:02:44,460] Trial 26 finished with value: 0.0016516620526090264 and parameters: {'learning_rate': 3.353225536336965e-05, 'num_layers': 3, 'hidden_size': 61, 'dropout_prob': 0.41892949105342375}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210244-4xo38w36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4xo38w36' target=\"_blank\">magic-jazz-237</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4xo38w36' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4xo38w36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 11.2 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▃▃▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss_epoch</td><td>0.00517</td></tr><tr><td>train_loss_step</td><td>0.00246</td></tr><tr><td>trainer/global_step</td><td>9730</td></tr><tr><td>val_loss</td><td>0.0017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-jazz-237</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4xo38w36' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4xo38w36</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210244-4xo38w36\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:03:58,869] Trial 27 finished with value: 0.0016961089568212628 and parameters: {'learning_rate': 1.0628451925176195e-05, 'num_layers': 2, 'hidden_size': 32, 'dropout_prob': 0.31575702387996324}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210358-uy1vmlb1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uy1vmlb1' target=\"_blank\">crimson-voice-238</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uy1vmlb1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uy1vmlb1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 56.2 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "56.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.2 K    Total params\n",
      "0.225     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃█▃▃▁▃▂▆▂▅▃▂▁▇▄▂▃▂▁▁▃▃▃▂▂█▃▂▁▂▃▁▂▃▃▁▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00114</td></tr><tr><td>trainer/global_step</td><td>8340</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-voice-238</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uy1vmlb1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uy1vmlb1</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210358-uy1vmlb1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:05:08,594] Trial 28 finished with value: 0.0016523663653060794 and parameters: {'learning_rate': 2.8806721781615465e-05, 'num_layers': 3, 'hidden_size': 59, 'dropout_prob': 0.3641570133699104}. Best is trial 21 with value: 0.001650880090892315.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210508-2vcc4j5j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2vcc4j5j' target=\"_blank\">fresh-field-239</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2vcc4j5j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2vcc4j5j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 14.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "14.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.6 K    Total params\n",
      "0.058     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁█▁▂▁▂▂▂▂▁▁▁▁▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00415</td></tr><tr><td>trainer/global_step</td><td>4448</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-field-239</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2vcc4j5j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/2vcc4j5j</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210508-2vcc4j5j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:06:00,424] Trial 29 finished with value: 0.0016484229126945138 and parameters: {'learning_rate': 0.00018887149084660608, 'num_layers': 2, 'hidden_size': 37, 'dropout_prob': 0.45255852589403345}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210600-21qy3ump</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/21qy3ump' target=\"_blank\">lyric-morning-240</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/21qy3ump' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/21qy3ump</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 3.3 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▃▆▃▂▁▂▂▆▂▃▄▂▁▆▃▂▃▂▁▂▂▂█▁▂▇▃▂▁▂▁▃▅▅▃▁▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00209</td></tr><tr><td>trainer/global_step</td><td>8201</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-morning-240</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/21qy3ump' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/21qy3ump</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210600-21qy3ump\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:07:10,570] Trial 30 finished with value: 0.0016568592982366681 and parameters: {'learning_rate': 0.0001994215945621308, 'num_layers': 2, 'hidden_size': 16, 'dropout_prob': 0.4505353233891298}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210710-aa36wi4j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa36wi4j' target=\"_blank\">volcanic-firebrand-241</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa36wi4j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa36wi4j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 11.8 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "11.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.8 K    Total params\n",
      "0.047     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▃▂▂▂▁▂▁▂▂▁▁▁▁▁▁▂▁▁▂▂▂▁▂▁▁▂█▂▂▁▁▂▂▁▁▁▁▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.0017</td></tr><tr><td>trainer/global_step</td><td>4309</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-firebrand-241</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa36wi4j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa36wi4j</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210710-aa36wi4j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:07:59,826] Trial 31 finished with value: 0.00166499731130898 and parameters: {'learning_rate': 5.433539066217505e-05, 'num_layers': 2, 'hidden_size': 33, 'dropout_prob': 0.49594971818173617}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210759-n988v59n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/n988v59n' target=\"_blank\">fresh-puddle-242</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/n988v59n' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/n988v59n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 15.3 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▂▂▂▁▁▂▁▁▂▁▁▂█▁▂▁▂▅▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▃▂▂▂▂▁▁▂▂▁▂▂▂▂▂▁▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00104</td></tr><tr><td>trainer/global_step</td><td>4031</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-puddle-242</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/n988v59n' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/n988v59n</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210759-n988v59n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:08:47,771] Trial 32 finished with value: 0.001657409011386335 and parameters: {'learning_rate': 0.00046937082807365786, 'num_layers': 2, 'hidden_size': 38, 'dropout_prob': 0.4478010906607739}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210847-j223dru7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/j223dru7' target=\"_blank\">gentle-eon-243</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/j223dru7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/j223dru7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.39919534237650645 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 12.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "12.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.9 K    Total params\n",
      "0.052     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▂▃▂▃▁▄█▁▅▁▄▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁█▇▂▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▄▆█▇▇▆▆▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>14</td></tr><tr><td>train_loss_epoch</td><td>0.00478</td></tr><tr><td>train_loss_step</td><td>0.00294</td></tr><tr><td>trainer/global_step</td><td>1946</td></tr><tr><td>val_loss</td><td>0.00171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-eon-243</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/j223dru7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/j223dru7</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210847-j223dru7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:09:25,819] Trial 33 finished with value: 0.0017109450418502092 and parameters: {'learning_rate': 0.0016821940939209027, 'num_layers': 1, 'hidden_size': 57, 'dropout_prob': 0.39919534237650645}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_210925-t8szmjwd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t8szmjwd' target=\"_blank\">laced-serenity-244</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t8szmjwd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t8szmjwd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 42.4 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "42.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.4 K    Total params\n",
      "0.170     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▂▇▃▃▄▂▃▂▆▂▂▄▄▂▂▁▆▄▁▅▃▂▁▂▁▂▂█▁▁▂▇▃▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00137</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-serenity-244</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t8szmjwd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/t8szmjwd</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_210925-t8szmjwd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:10:21,858] Trial 34 finished with value: 0.0016555794281885028 and parameters: {'learning_rate': 3.62738151046209e-05, 'num_layers': 3, 'hidden_size': 51, 'dropout_prob': 0.350362983850934}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211021-44rt6mpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/44rt6mpi' target=\"_blank\">icy-fire-245</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/44rt6mpi' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/44rt6mpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 7.7 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "7.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▂█▁▂▁▁▂▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00227</td></tr><tr><td>trainer/global_step</td><td>4170</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-fire-245</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/44rt6mpi' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/44rt6mpi</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211021-44rt6mpi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:11:11,755] Trial 35 finished with value: 0.001654253457672894 and parameters: {'learning_rate': 0.00014962368848019252, 'num_layers': 2, 'hidden_size': 26, 'dropout_prob': 0.17866562129608968}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211111-l81bkcbs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/l81bkcbs' target=\"_blank\">vital-eon-246</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/l81bkcbs' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/l81bkcbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.418487950309722 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 16.3 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "16.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.3 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂█▁▆▃▂▂▂▅▂▁▇▁▆▃▁▃▃▃▁▂█▃▁▅▁▃▆▃▃▂▃▂▂▇▃▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss_epoch</td><td>0.00478</td></tr><tr><td>train_loss_step</td><td>0.00186</td></tr><tr><td>trainer/global_step</td><td>9730</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-eon-246</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/l81bkcbs' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/l81bkcbs</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211111-l81bkcbs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:12:25,202] Trial 36 finished with value: 0.0016574442852288485 and parameters: {'learning_rate': 2.3748997227932488e-05, 'num_layers': 1, 'hidden_size': 65, 'dropout_prob': 0.418487950309722}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211225-0q8dtkvg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0q8dtkvg' target=\"_blank\">lyric-dragon-247</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0q8dtkvg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0q8dtkvg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 29.3 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "29.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.3 K    Total params\n",
      "0.117     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▂▃▃▃▁▄█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▂▁▂▄▄▃▃█▆▇▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00158</td></tr><tr><td>trainer/global_step</td><td>1668</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-dragon-247</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0q8dtkvg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0q8dtkvg</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211225-0q8dtkvg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:13:03,231] Trial 37 finished with value: 0.00166137027554214 and parameters: {'learning_rate': 0.0005164764503144299, 'num_layers': 3, 'hidden_size': 42, 'dropout_prob': 0.4613213381543821}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211303-qzvt9dsp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qzvt9dsp' target=\"_blank\">whole-totem-248</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qzvt9dsp' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qzvt9dsp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 98.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "98.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.9 K    Total params\n",
      "0.396     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▆▂▃▃▁▃█▁▅▁▂▁▃▃▃▄▁▄▃▂▃▃▂▂▂▆██▂▅▂▃▁▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▂▁▂▃▃▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00201</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-totem-248</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qzvt9dsp' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/qzvt9dsp</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211303-qzvt9dsp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:13:43,319] Trial 38 finished with value: 0.001652300707064569 and parameters: {'learning_rate': 0.0017102408492028624, 'num_layers': 3, 'hidden_size': 79, 'dropout_prob': 0.405572004076656}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211343-slg8s4y8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/slg8s4y8' target=\"_blank\">cool-firebrand-249</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/slg8s4y8' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/slg8s4y8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 75.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "75.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.1 K    Total params\n",
      "0.300     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▂▂▁▂▂▁▂▂▂▁▁▂▁▁▁▂█▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00091</td></tr><tr><td>trainer/global_step</td><td>3336</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-firebrand-249</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/slg8s4y8' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/slg8s4y8</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211343-slg8s4y8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:14:26,556] Trial 39 finished with value: 0.0016574705950915813 and parameters: {'learning_rate': 6.291680526679318e-05, 'num_layers': 2, 'hidden_size': 88, 'dropout_prob': 0.132604315459333}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211426-tukakh53</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tukakh53' target=\"_blank\">upbeat-frost-250</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tukakh53' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tukakh53</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 172 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "172 K     Trainable params\n",
      "0         Non-trainable params\n",
      "172 K     Total params\n",
      "0.690     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▂▇▂▃▄▁▄▁▅▁▄▁▃▃▃▄▁▄▃▃▁▃▂▂▆▂█▂▅▂▄▁▄▃▆▅▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>train_loss_epoch</td><td>0.00487</td></tr><tr><td>train_loss_step</td><td>0.00265</td></tr><tr><td>trainer/global_step</td><td>2502</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-frost-250</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tukakh53' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tukakh53</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211426-tukakh53\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:15:06,429] Trial 40 finished with value: 0.0016518638003617525 and parameters: {'learning_rate': 2.2244248431728537e-05, 'num_layers': 3, 'hidden_size': 105, 'dropout_prob': 0.3802777989867408}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211506-v20aarw6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v20aarw6' target=\"_blank\">clean-firebrand-251</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v20aarw6' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v20aarw6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 185 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "185 K     Trainable params\n",
      "0         Non-trainable params\n",
      "185 K     Total params\n",
      "0.742     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▂▅▁▁▂▁▁▂▂▅▃▂▂▃▂▂▁▂▂▃▃▄▂▂▂▄█▂▁▂▁▁▂▂▄▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▅▃▄▂▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>43</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00254</td></tr><tr><td>trainer/global_step</td><td>5977</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-firebrand-251</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v20aarw6' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/v20aarw6</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211506-v20aarw6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:16:07,533] Trial 41 finished with value: 0.0016524572856724262 and parameters: {'learning_rate': 1.8889082988476546e-05, 'num_layers': 3, 'hidden_size': 109, 'dropout_prob': 0.3791769281385662}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211607-yf2knuhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yf2knuhq' target=\"_blank\">dulcet-monkey-252</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yf2knuhq' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yf2knuhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 166 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "166 K     Trainable params\n",
      "0         Non-trainable params\n",
      "166 K     Total params\n",
      "0.664     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▆▃▁█▅▂▃▆▁▃▁▂▆█▂▄▁▃▅▃▅▂▂▃█▄▃▄▆▄▄▃▂▅▃▂▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00463</td></tr><tr><td>trainer/global_step</td><td>4726</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-monkey-252</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yf2knuhq' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/yf2knuhq</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211607-yf2knuhq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:16:59,399] Trial 42 finished with value: 0.0016549777938053012 and parameters: {'learning_rate': 2.231731945056802e-05, 'num_layers': 3, 'hidden_size': 103, 'dropout_prob': 0.33854642905759963}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211659-0bfkk26r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0bfkk26r' target=\"_blank\">glad-cloud-253</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0bfkk26r' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0bfkk26r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 199 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "199 K     Trainable params\n",
      "0         Non-trainable params\n",
      "199 K     Total params\n",
      "0.796     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃▂▁▄▁▁▂▃▂▁▂▁▁▃▂▁▂▂▁▁▁▁▂▃▃▂▂█▂▂▂▁▃▂▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_loss_epoch</td><td>0.00487</td></tr><tr><td>train_loss_step</td><td>0.00107</td></tr><tr><td>trainer/global_step</td><td>4865</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-cloud-253</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0bfkk26r' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/0bfkk26r</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211659-0bfkk26r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:17:51,153] Trial 43 finished with value: 0.0016555662732571363 and parameters: {'learning_rate': 1.1919670021246937e-05, 'num_layers': 3, 'hidden_size': 113, 'dropout_prob': 0.42616183646010675}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211751-3ejix1bt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ejix1bt' target=\"_blank\">golden-resonance-254</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ejix1bt' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ejix1bt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 114 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "114 K     Trainable params\n",
      "0         Non-trainable params\n",
      "114 K     Total params\n",
      "0.456     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▅▂▁▁▁▁▃▃▂▂▂▄▆▄▃▃▄▂▂▂▂▃▆▃▃▂▅▂▂▃▂▂▁▃▁▃█▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▄▃▂▂▂▂▂▁▂▂▂▁▂▁▂▂▂▂▁▂▂▁▁▂▁▂▁▂▂▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00203</td></tr><tr><td>trainer/global_step</td><td>5143</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-resonance-254</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ejix1bt' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ejix1bt</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211751-3ejix1bt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:18:40,491] Trial 44 finished with value: 0.0016557839699089527 and parameters: {'learning_rate': 3.103344496736727e-05, 'num_layers': 3, 'hidden_size': 85, 'dropout_prob': 0.47442817836302204}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211840-doq3bgxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/doq3bgxb' target=\"_blank\">noble-dawn-255</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/doq3bgxb' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/doq3bgxb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 122 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "122 K     Trainable params\n",
      "0         Non-trainable params\n",
      "122 K     Total params\n",
      "0.488     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▂▃▄▂▄▂▅▂▃▁▃▃▃▄▁▄▃▃▁▃▂▂▇▁█▂▅▂▄▁▄▃▆▅▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▁▁▁▂▂▂▂▁▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>train_loss_epoch</td><td>0.00486</td></tr><tr><td>train_loss_step</td><td>0.00248</td></tr><tr><td>trainer/global_step</td><td>2502</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-dawn-255</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/doq3bgxb' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/doq3bgxb</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211840-doq3bgxb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:19:19,930] Trial 45 finished with value: 0.0016539293574169278 and parameters: {'learning_rate': 6.276950120111782e-05, 'num_layers': 3, 'hidden_size': 88, 'dropout_prob': 0.2699334416826522}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211919-aho24vgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aho24vgy' target=\"_blank\">valiant-glitter-256</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aho24vgy' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aho24vgy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 84.8 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "84.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.8 K    Total params\n",
      "0.339     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▆▂▃▃▃▁▄█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▄▁▁▃▆▆▆▆██▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.0015</td></tr><tr><td>trainer/global_step</td><td>1668</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-glitter-256</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aho24vgy' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aho24vgy</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211919-aho24vgy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:19:59,920] Trial 46 finished with value: 0.0016609749291092157 and parameters: {'learning_rate': 0.00011815496568385209, 'num_layers': 3, 'hidden_size': 73, 'dropout_prob': 0.007927362872565402}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_211959-fbvl8txb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fbvl8txb' target=\"_blank\">worthy-wood-257</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fbvl8txb' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fbvl8txb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 98.0 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "98.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.0 K    Total params\n",
      "0.392     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▆▃▂▅▂▂▁▆▂▂▅▄▂▂▁▆▃▁▅▃▂▁▂▁▃▂█▁▁▂▆▃▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00133</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-wood-257</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fbvl8txb' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fbvl8txb</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_211959-fbvl8txb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:21:04,427] Trial 47 finished with value: 0.0016521806828677654 and parameters: {'learning_rate': 1.5320512613022402e-05, 'num_layers': 2, 'hidden_size': 101, 'dropout_prob': 0.3865102646037035}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212104-ekc12e81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ekc12e81' target=\"_blank\">resilient-lion-258</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ekc12e81' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ekc12e81</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 182 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "182 K     Trainable params\n",
      "0         Non-trainable params\n",
      "182 K     Total params\n",
      "0.729     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▆▂▃▃▄▁▄█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▂██▂▅▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▇▃▃▁▃▆▄▆▇██▆▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.0009</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-lion-258</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ekc12e81' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ekc12e81</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212104-ekc12e81\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:21:44,383] Trial 48 finished with value: 0.001670327503234148 and parameters: {'learning_rate': 0.0010198357705292441, 'num_layers': 3, 'hidden_size': 108, 'dropout_prob': 0.34737139866433325}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212144-7988ox5o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7988ox5o' target=\"_blank\">treasured-shape-259</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7988ox5o' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7988ox5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2151352219375826 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 11.0 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "11.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.0 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▇▂▃▂▃▁▄█▁▅▁▄▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁█▇▂▅▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▃▄▅▆▆▆▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00473</td></tr><tr><td>train_loss_step</td><td>0.00095</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00173</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-shape-259</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7988ox5o' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7988ox5o</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212144-7988ox5o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:22:24,367] Trial 49 finished with value: 0.0017296351725235581 and parameters: {'learning_rate': 0.003021736315133757, 'num_layers': 1, 'hidden_size': 52, 'dropout_prob': 0.2151352219375826}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212224-fh3uem7t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fh3uem7t' target=\"_blank\">stoic-fog-260</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fh3uem7t' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fh3uem7t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 227 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "227 K     Trainable params\n",
      "0         Non-trainable params\n",
      "227 K     Total params\n",
      "0.911     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▆▃▃█▅▃▃▃▄▂▂▃▁▁█▅▂▄▆▁▂▄▂▁▂█▄▃▃▆▄▃▂▂▁▃▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▇██▅▄▃▄▄▄▃▃▃▃▄▄▄▃▃▃▃▃▂▁▂▂▂▃▄▃▂▃▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00086</td></tr><tr><td>trainer/global_step</td><td>4587</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-fog-260</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fh3uem7t' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/fh3uem7t</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212224-fh3uem7t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:23:20,295] Trial 50 finished with value: 0.0016542695229873061 and parameters: {'learning_rate': 0.00017983214836313804, 'num_layers': 3, 'hidden_size': 121, 'dropout_prob': 0.319719821597044}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212320-a6zg70nx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/a6zg70nx' target=\"_blank\">neat-paper-261</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/a6zg70nx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/a6zg70nx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 99.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "99.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.9 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▁▁▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▂▂▁▂▂▁▂▂▂▁▁▂▁▁▁▂█▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24</td></tr><tr><td>train_loss_epoch</td><td>0.0049</td></tr><tr><td>train_loss_step</td><td>0.00101</td></tr><tr><td>trainer/global_step</td><td>3336</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-paper-261</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/a6zg70nx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/a6zg70nx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212320-a6zg70nx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:24:08,368] Trial 51 finished with value: 0.0016511677531525493 and parameters: {'learning_rate': 1.6016351899634368e-05, 'num_layers': 2, 'hidden_size': 102, 'dropout_prob': 0.38754016747679476}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212408-5jakurw2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5jakurw2' target=\"_blank\">frosty-oath-262</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5jakurw2' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5jakurw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 80.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "80.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.1 K    Total params\n",
      "0.320     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▁▁▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁▁▂▁▁▂█▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.00494</td></tr><tr><td>train_loss_step</td><td>0.00519</td></tr><tr><td>trainer/global_step</td><td>3614</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-oath-262</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5jakurw2' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5jakurw2</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212408-5jakurw2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:24:56,277] Trial 52 finished with value: 0.0016541865188628435 and parameters: {'learning_rate': 1.0172950078911989e-05, 'num_layers': 2, 'hidden_size': 91, 'dropout_prob': 0.4364077184225815}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212456-aa3qn2oe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa3qn2oe' target=\"_blank\">rosy-microwave-263</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa3qn2oe' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa3qn2oe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 109 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃▂▁▄▁▂▂▃▂▁▂▁▁▃▃▂▂▃▁▁▁▁▂▄▃▂▂█▂▂▂▁▃▂▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_loss_epoch</td><td>0.00489</td></tr><tr><td>train_loss_step</td><td>0.00106</td></tr><tr><td>trainer/global_step</td><td>4865</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-microwave-263</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa3qn2oe' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/aa3qn2oe</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212456-aa3qn2oe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:25:52,998] Trial 53 finished with value: 0.001652942388318479 and parameters: {'learning_rate': 1.685154807077451e-05, 'num_layers': 2, 'hidden_size': 107, 'dropout_prob': 0.40719748246829723}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212553-tqefky9r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tqefky9r' target=\"_blank\">exalted-paper-264</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tqefky9r' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tqefky9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 96.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "96.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "96.1 K    Total params\n",
      "0.384     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▁▂▂▂▁▁▂▁▂▃▂▂▁▁▂▁▃▃▄▃▂▁▂▃▂▂▂▂▁▁▁▂▄▃▂▂▂█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.0049</td></tr><tr><td>train_loss_step</td><td>0.02973</td></tr><tr><td>trainer/global_step</td><td>3475</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-paper-264</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tqefky9r' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tqefky9r</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212553-tqefky9r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:26:41,293] Trial 54 finished with value: 0.0016488807741552591 and parameters: {'learning_rate': 3.1605947352766966e-05, 'num_layers': 2, 'hidden_size': 100, 'dropout_prob': 0.3602081055097981}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212641-nk8kyy12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/nk8kyy12' target=\"_blank\">expert-disco-265</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/nk8kyy12' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/nk8kyy12</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 94.2 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▁▇▂▃▃▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁█▇▂▅▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▃▁▂▂▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00095</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-disco-265</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/nk8kyy12' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/nk8kyy12</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212641-nk8kyy12\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:27:27,628] Trial 55 finished with value: 0.0016525456449016929 and parameters: {'learning_rate': 7.54022747724497e-05, 'num_layers': 2, 'hidden_size': 99, 'dropout_prob': 0.3602517357997577}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212727-srzjlkw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/srzjlkw1' target=\"_blank\">logical-wood-266</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/srzjlkw1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/srzjlkw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 64.0 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "64.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "64.0 K    Total params\n",
      "0.256     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▃▃▃▃▂▄█▂▆▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁██▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▂▂▂▂▃▄▄▄▅▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00223</td></tr><tr><td>trainer/global_step</td><td>1807</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-wood-266</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/srzjlkw1' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/srzjlkw1</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212727-srzjlkw1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:28:07,491] Trial 56 finished with value: 0.0016524519305676222 and parameters: {'learning_rate': 5.0993369141395746e-05, 'num_layers': 2, 'hidden_size': 81, 'dropout_prob': 0.46982531503145913}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212807-5zfib63y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5zfib63y' target=\"_blank\">jolly-fog-267</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5zfib63y' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5zfib63y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 83.5 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "83.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "83.5 K    Total params\n",
      "0.334     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▂▃▁▁▂▂▁▁▁▁▂▁▂▁▁▁█▂▁▁▂▁▂▁▄▁▁▂▁▁▁▂▃▂▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>46</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00157</td></tr><tr><td>trainer/global_step</td><td>6394</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-fog-267</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5zfib63y' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5zfib63y</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212807-5zfib63y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:29:09,634] Trial 57 finished with value: 0.0016525537939742208 and parameters: {'learning_rate': 3.2280910610207006e-05, 'num_layers': 2, 'hidden_size': 93, 'dropout_prob': 0.4318991696753537}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212909-lo2ve9wk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/lo2ve9wk' target=\"_blank\">dutiful-gorge-268</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/lo2ve9wk' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/lo2ve9wk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 119 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "119 K     Trainable params\n",
      "0         Non-trainable params\n",
      "119 K     Total params\n",
      "0.479     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▇▂▃▃▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁▇▇▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▂▄▃▃▃▄▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00222</td></tr><tr><td>trainer/global_step</td><td>1807</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-gorge-268</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/lo2ve9wk' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/lo2ve9wk</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212909-lo2ve9wk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:29:47,586] Trial 58 finished with value: 0.001654412946663797 and parameters: {'learning_rate': 9.28983995980189e-05, 'num_layers': 2, 'hidden_size': 112, 'dropout_prob': 0.29390191348139844}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_212947-r6izwmjr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r6izwmjr' target=\"_blank\">clean-vortex-269</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r6izwmjr' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r6izwmjr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 20.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "20.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.1 K    Total params\n",
      "0.080     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▆▃▂▅▂▂▂▆▂▂▅▄▂▂▁▆▃▁▅▃▂▁▂▁▃▂█▁▁▂▇▃▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00128</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-vortex-269</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r6izwmjr' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r6izwmjr</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_212947-r6izwmjr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:30:49,874] Trial 59 finished with value: 0.0016501386417075992 and parameters: {'learning_rate': 3.098686729990362e-05, 'num_layers': 2, 'hidden_size': 44, 'dropout_prob': 0.04644279295644599}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213049-xp3x32gx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xp3x32gx' target=\"_blank\">astral-monkey-270</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xp3x32gx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xp3x32gx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 7.7 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "7.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▂▁▂▁▂█▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▂▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▇▄▅▆▅▆▇▄▅▃▃▂▃▂▂▃▃█▂▂▃▂▂▃▃▃▂▄▂▁▂▃▂▃▃▂▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_loss_epoch</td><td>0.00487</td></tr><tr><td>train_loss_step</td><td>0.00157</td></tr><tr><td>trainer/global_step</td><td>5838</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-monkey-270</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xp3x32gx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/xp3x32gx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213049-xp3x32gx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:31:53,660] Trial 60 finished with value: 0.0016556537011638284 and parameters: {'learning_rate': 0.05002438154861314, 'num_layers': 2, 'hidden_size': 26, 'dropout_prob': 0.039145201350289274}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213153-pdgnqk00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/pdgnqk00' target=\"_blank\">frosty-durian-271</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/pdgnqk00' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/pdgnqk00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 21.9 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "21.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.9 K    Total params\n",
      "0.087     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▃▁▅▁▆▂▃▂█▂▄▅▂▂▂█▄▃▄▃▃▅▂▃▃▃▁▂▁▃▁▁▁▃▁▄▆▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>53</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00121</td></tr><tr><td>trainer/global_step</td><td>7367</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-durian-271</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/pdgnqk00' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/pdgnqk00</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213153-pdgnqk00\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:33:00,365] Trial 61 finished with value: 0.001649850164540112 and parameters: {'learning_rate': 4.569904290035125e-05, 'num_layers': 2, 'hidden_size': 46, 'dropout_prob': 0.148024690913914}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213300-p551xjqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/p551xjqg' target=\"_blank\">stoic-pond-272</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/p551xjqg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/p551xjqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 17.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "17.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.6 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▂▂▃▁▂▂▂▂▁▃▃▁▁▃▃▁▂▁▁▂▂▂▇▂▁▂▁▂▂▂▂▁▂▁▁▂▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00884</td></tr><tr><td>trainer/global_step</td><td>5699</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-pond-272</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/p551xjqg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/p551xjqg</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213300-p551xjqg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:33:58,641] Trial 62 finished with value: 0.0016578974900767207 and parameters: {'learning_rate': 4.610470514926918e-05, 'num_layers': 2, 'hidden_size': 41, 'dropout_prob': 0.12855493599841772}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213358-x0pek8av</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x0pek8av' target=\"_blank\">peach-flower-273</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x0pek8av' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x0pek8av</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 20.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "20.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.1 K    Total params\n",
      "0.080     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▄▁▅▂▃▅▂▂▃▄▂▂▂▄▁▅▃▂▁▂▃▂█▁▂▁▃▃▂▁▂▁▃▅▅▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>54</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00139</td></tr><tr><td>trainer/global_step</td><td>7506</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-flower-273</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x0pek8av' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x0pek8av</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213358-x0pek8av\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:35:07,435] Trial 63 finished with value: 0.0016520104836672544 and parameters: {'learning_rate': 2.641875277980342e-05, 'num_layers': 2, 'hidden_size': 44, 'dropout_prob': 0.06756893730860748}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213507-3bu6zdp9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3bu6zdp9' target=\"_blank\">electric-feather-274</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3bu6zdp9' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3bu6zdp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 14.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "14.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.6 K    Total params\n",
      "0.058     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▅▁▆▂▃▂█▂▄▅▂▂▂█▄▃▄▄▄▆▂▃▄▃▂▂▁▃▂▁▁▃▁▄▆▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>53</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00118</td></tr><tr><td>trainer/global_step</td><td>7367</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-feather-274</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3bu6zdp9' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3bu6zdp9</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213507-3bu6zdp9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:36:14,992] Trial 64 finished with value: 0.0016557471826672554 and parameters: {'learning_rate': 1.4301368688272382e-05, 'num_layers': 2, 'hidden_size': 37, 'dropout_prob': 0.08458998590307167}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213615-r3r5akqs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r3r5akqs' target=\"_blank\">trim-sound-275</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r3r5akqs' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r3r5akqs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 30.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "30.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.6 K    Total params\n",
      "0.122     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▂▂▁▁▂▁▁▂▁▁▂▁▁▂█▁▂▁▂▅▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00359</td></tr><tr><td>trainer/global_step</td><td>3892</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sound-275</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r3r5akqs' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r3r5akqs</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213615-r3r5akqs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:37:04,142] Trial 65 finished with value: 0.0016558495117351413 and parameters: {'learning_rate': 0.00012948243872610104, 'num_layers': 2, 'hidden_size': 55, 'dropout_prob': 0.23221559291480356}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213704-wg4bwfmn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wg4bwfmn' target=\"_blank\">rich-elevator-276</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wg4bwfmn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wg4bwfmn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 9.3 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "9.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 K     Total params\n",
      "0.037     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▁▁▁▃▂▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▁▂█▁▂▁▂▅▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00104</td></tr><tr><td>trainer/global_step</td><td>3753</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-elevator-276</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wg4bwfmn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wg4bwfmn</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213704-wg4bwfmn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:37:52,252] Trial 66 finished with value: 0.0016529979184269905 and parameters: {'learning_rate': 6.680292021740483e-05, 'num_layers': 2, 'hidden_size': 29, 'dropout_prob': 0.1470812398972101}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213752-x8b8vpvg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x8b8vpvg' target=\"_blank\">warm-forest-277</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x8b8vpvg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x8b8vpvg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 23.7 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "23.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 K    Total params\n",
      "0.095     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▂▅▁▁▂▁▁▂▂▅▃▂▂▃▂▂▁▂▂▃▂▄▂▂▂▄█▂▁▂▁▁▂▂▄▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>43</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.00256</td></tr><tr><td>trainer/global_step</td><td>5977</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-forest-277</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x8b8vpvg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x8b8vpvg</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213752-x8b8vpvg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:38:50,389] Trial 67 finished with value: 0.001653767074458301 and parameters: {'learning_rate': 4.571658643309081e-05, 'num_layers': 2, 'hidden_size': 48, 'dropout_prob': 0.042696560976795174}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_213850-ejst7z67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ejst7z67' target=\"_blank\">confused-resonance-278</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ejst7z67' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ejst7z67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 5.3 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃▇▁▅▃▂▂▂▅▂▂▇▁▅▃▁▃▃▃▂▂█▃▁▅▁▃▆▃▃▂▃▂▂▆▃▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.00193</td></tr><tr><td>trainer/global_step</td><td>9730</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-resonance-278</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ejst7z67' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ejst7z67</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_213850-ejst7z67\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:40:09,101] Trial 68 finished with value: 0.0016599813243374228 and parameters: {'learning_rate': 3.688549550464228e-05, 'num_layers': 2, 'hidden_size': 21, 'dropout_prob': 0.11281267026645145}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214009-1vfnd3v0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1vfnd3v0' target=\"_blank\">vague-wave-279</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1vfnd3v0' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1vfnd3v0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 90.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "90.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.6 K    Total params\n",
      "0.362     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▁▂▂▂▁▁▂▁▂▃▂▂▁▁▂▁▃▄▄▃▂▁▂▃▃▂▂▂▂▁▁▂▄▃▂▂▂█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▃▂▂▃▂▂▂▁▂▃▂▂▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.02869</td></tr><tr><td>trainer/global_step</td><td>3475</td></tr><tr><td>val_loss</td><td>0.00167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-wave-279</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1vfnd3v0' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/1vfnd3v0</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214009-1vfnd3v0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:40:55,232] Trial 69 finished with value: 0.001669069635681808 and parameters: {'learning_rate': 9.154665944536204e-05, 'num_layers': 2, 'hidden_size': 97, 'dropout_prob': 0.026018255384731924}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214055-iz8y5g3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/iz8y5g3q' target=\"_blank\">celestial-dew-280</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/iz8y5g3q' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/iz8y5g3q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1995408978109452 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 18.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "18.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.6 K    Total params\n",
      "0.074     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▇▂▃▃▃▁▃█▁▅▁▄▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁██▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▁▂▂▃▄▄▅▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00216</td></tr><tr><td>trainer/global_step</td><td>1807</td></tr><tr><td>val_loss</td><td>0.00168</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-dew-280</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/iz8y5g3q' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/iz8y5g3q</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214055-iz8y5g3q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:41:32,948] Trial 70 finished with value: 0.0016800696030259132 and parameters: {'learning_rate': 0.00021833555118226075, 'num_layers': 1, 'hidden_size': 70, 'dropout_prob': 0.1995408978109452}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214132-bn30koy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/bn30koy7' target=\"_blank\">rose-sound-281</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/bn30koy7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/bn30koy7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 39.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "39.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.6 K    Total params\n",
      "0.158     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▂▁▁▁▁▂▂▂▂▁▃▄▃▂▂▃▁▂▂▁▂▄▂▂▂▃▁▂▂▁▁▁▂▁▂▅▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_loss_epoch</td><td>0.00486</td></tr><tr><td>train_loss_step</td><td>0.00187</td></tr><tr><td>trainer/global_step</td><td>5143</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-sound-281</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/bn30koy7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/bn30koy7</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214132-bn30koy7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:42:29,184] Trial 71 finished with value: 0.0016539313364773989 and parameters: {'learning_rate': 2.755851575607204e-05, 'num_layers': 2, 'hidden_size': 63, 'dropout_prob': 0.15900455564977606}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214229-3ruxk5ow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ruxk5ow' target=\"_blank\">colorful-hill-282</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ruxk5ow' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ruxk5ow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 16.1 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "16.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.1 K    Total params\n",
      "0.064     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▃█▄▃▁▂▂▇▂▃▅▂▂▂▆▁▆▃▃▅▁▃▃▃▂▂█▃▂▁▃▁▃▅▆▃▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>58</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00082</td></tr><tr><td>trainer/global_step</td><td>8062</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-hill-282</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ruxk5ow' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/3ruxk5ow</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214229-3ruxk5ow\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:43:39,787] Trial 72 finished with value: 0.0016600011149421334 and parameters: {'learning_rate': 1.919292709800381e-05, 'num_layers': 2, 'hidden_size': 39, 'dropout_prob': 0.4114584472397965}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214339-gfrtuf3n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/gfrtuf3n' target=\"_blank\">astral-smoke-283</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/gfrtuf3n' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/gfrtuf3n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 23.7 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "23.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 K    Total params\n",
      "0.095     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▂▁▁▃▁▁▁▂▁▂▁▂▁▁▁▁▁▂▂▁▁▂▂▂▁▁▂▁▁▂█▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.00487</td></tr><tr><td>train_loss_step</td><td>0.00519</td></tr><tr><td>trainer/global_step</td><td>3614</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-smoke-283</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/gfrtuf3n' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/gfrtuf3n</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214339-gfrtuf3n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:44:27,786] Trial 73 finished with value: 0.0016525662504136562 and parameters: {'learning_rate': 3.9613787551944634e-05, 'num_layers': 2, 'hidden_size': 48, 'dropout_prob': 0.3940491114249912}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214427-41tmz6gf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/41tmz6gf' target=\"_blank\">royal-night-284</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/41tmz6gf' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/41tmz6gf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 12.5 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▇▃▅▂▃▆▂▃▂▂▁▆▁▅▃▁▂▂▃▂▁▁▃▁▁▂▁▃▂▂▃▂▃▃▂▁▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>66</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00857</td></tr><tr><td>trainer/global_step</td><td>9174</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-night-284</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/41tmz6gf' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/41tmz6gf</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214427-41tmz6gf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:45:43,621] Trial 74 finished with value: 0.0016594097251072526 and parameters: {'learning_rate': 1.3341040835229686e-05, 'num_layers': 2, 'hidden_size': 34, 'dropout_prob': 0.4457454442937041}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214543-s6mhm9uh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/s6mhm9uh' target=\"_blank\">crisp-silence-285</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/s6mhm9uh' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/s6mhm9uh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 43.2 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "43.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.2 K    Total params\n",
      "0.173     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇█▃▁▅▄▂▂▄▁▂▁▁▄▅▂▃▁▂▃▂▃▂▁▂▅▃▂▃▄▂▂▂▂▃▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_loss_epoch</td><td>0.00488</td></tr><tr><td>train_loss_step</td><td>0.00499</td></tr><tr><td>trainer/global_step</td><td>4726</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-silence-285</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/s6mhm9uh' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/s6mhm9uh</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214543-s6mhm9uh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:46:37,722] Trial 75 finished with value: 0.001648587640374899 and parameters: {'learning_rate': 2.956270037508475e-05, 'num_layers': 2, 'hidden_size': 66, 'dropout_prob': 0.36662485623029367}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214637-r4618vzw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r4618vzw' target=\"_blank\">robust-darkness-286</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r4618vzw' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r4618vzw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 56.6 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "56.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.6 K    Total params\n",
      "0.226     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▂▂▂▂▁▁▂▁▂▃▂▂▁▁▂▁▃▃▃▃▂▁▂▃▃▂▂▂▁▁▁▂▄▃▂▂▂█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.02893</td></tr><tr><td>trainer/global_step</td><td>3475</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-darkness-286</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r4618vzw' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/r4618vzw</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214637-r4618vzw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 21:47:56,948] Trial 76 finished with value: 0.0016519135097041726 and parameters: {'learning_rate': 5.819779735441163e-05, 'num_layers': 2, 'hidden_size': 76, 'dropout_prob': 0.31132021345107075}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_214756-kygfowbd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/kygfowbd' target=\"_blank\">eager-shape-287</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/kygfowbd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/kygfowbd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 21.0 K\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "21.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.0 K    Total params\n",
      "0.084     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▂█▁▅▃▁▂▂▄▂▂▇▁▅▃▁▃▃▃▁▂▇▃▁▅▁▃▅▂▂▂▃▁▂▆▃▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00198</td></tr><tr><td>trainer/global_step</td><td>9730</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-shape-287</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/kygfowbd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/kygfowbd</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_214756-kygfowbd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:40:16,339] Trial 77 finished with value: 0.0016529038548469543 and parameters: {'learning_rate': 1.9779720351423417e-05, 'num_layers': 2, 'hidden_size': 45, 'dropout_prob': 0.3330343760743106}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224016-ys1f74he</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ys1f74he' target=\"_blank\">noble-glitter-288</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ys1f74he' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ys1f74he</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 155 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "155 K     Trainable params\n",
      "0         Non-trainable params\n",
      "155 K     Total params\n",
      "0.621     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▂▁▃▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▂▂▁▂▂▂▁▁▁▂▁▁▁▂█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▂▁▂▁▂▁▁▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00314</td></tr><tr><td>trainer/global_step</td><td>3058</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-glitter-288</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ys1f74he' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ys1f74he</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224016-ys1f74he\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:40:59,313] Trial 78 finished with value: 0.0016506633255630732 and parameters: {'learning_rate': 2.76367237361751e-05, 'num_layers': 2, 'hidden_size': 128, 'dropout_prob': 0.3699138791648888}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224059-g3uzl0yd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/g3uzl0yd' target=\"_blank\">sandy-darkness-289</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/g3uzl0yd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/g3uzl0yd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 155 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "155 K     Trainable params\n",
      "0         Non-trainable params\n",
      "155 K     Total params\n",
      "0.621     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▃▃▂▄▂▅▄▂▃▃▆▄▁▂▂▁▃▂▂▁█▂▅▄▃▁▃▆▆▂▃▂▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▂▁▂▂▁▁▁▁▁▂▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>21</td></tr><tr><td>train_loss_epoch</td><td>0.00485</td></tr><tr><td>train_loss_step</td><td>0.00298</td></tr><tr><td>trainer/global_step</td><td>2919</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-darkness-289</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/g3uzl0yd' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/g3uzl0yd</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224059-g3uzl0yd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:41:43,064] Trial 79 finished with value: 0.0016564317047595978 and parameters: {'learning_rate': 2.607009066494607e-05, 'num_layers': 2, 'hidden_size': 128, 'dropout_prob': 0.272367044345985}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224143-763vj1sc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/763vj1sc' target=\"_blank\">visionary-resonance-290</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/763vj1sc' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/763vj1sc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 126 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "126 K     Trainable params\n",
      "0         Non-trainable params\n",
      "126 K     Total params\n",
      "0.504     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▂▃▃▁█▁▅▄▂▁▃▃▆▄▄▃▂▁▃▂▂▆▂█▂▅▂▂▁▄▆▅▅▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▂▂▁▁▃▂▃▂▂▁▁▄▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>0.00481</td></tr><tr><td>train_loss_step</td><td>0.00263</td></tr><tr><td>trainer/global_step</td><td>2641</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-resonance-290</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/763vj1sc' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/763vj1sc</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224143-763vj1sc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:42:25,968] Trial 80 finished with value: 0.001651483355090022 and parameters: {'learning_rate': 7.690864682725776e-05, 'num_layers': 2, 'hidden_size': 115, 'dropout_prob': 0.4846366901540457}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224226-769aolga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/769aolga' target=\"_blank\">logical-plant-291</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/769aolga' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/769aolga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 136 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.548     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▆▃▃▄▄█▆▂▄▁▃▃▆▄▄▃▂▁▃▂▂▆██▅▂▄▁▄▃▅▆▃▃▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00193</td></tr><tr><td>trainer/global_step</td><td>2780</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-plant-291</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/769aolga' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/769aolga</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224226-769aolga\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:43:09,554] Trial 81 finished with value: 0.0016486775130033493 and parameters: {'learning_rate': 3.548515482601102e-05, 'num_layers': 2, 'hidden_size': 120, 'dropout_prob': 0.3669872460263737}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224309-ezstdp5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ezstdp5d' target=\"_blank\">crisp-serenity-292</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ezstdp5d' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ezstdp5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 136 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.548     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▂█▁▂▁▁▂▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00214</td></tr><tr><td>trainer/global_step</td><td>4170</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-serenity-292</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ezstdp5d' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/ezstdp5d</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224309-ezstdp5d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:43:59,713] Trial 82 finished with value: 0.0016507036052644253 and parameters: {'learning_rate': 3.264263300112418e-05, 'num_layers': 2, 'hidden_size': 120, 'dropout_prob': 0.3684678208717457}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224359-m3rxc7f7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/m3rxc7f7' target=\"_blank\">confused-dream-293</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/m3rxc7f7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/m3rxc7f7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 150 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "150 K     Trainable params\n",
      "0         Non-trainable params\n",
      "150 K     Total params\n",
      "0.602     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▆▂▃▃▁▃▇▁▅▁▂▁▃▃▃▄▁▄▂▂▃▃▂▂▂▅▇▇▂▄▂▃▁▃▃▅▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00486</td></tr><tr><td>train_loss_step</td><td>0.00205</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-dream-293</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/m3rxc7f7' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/m3rxc7f7</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224359-m3rxc7f7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:44:40,702] Trial 83 finished with value: 0.0016494306037202477 and parameters: {'learning_rate': 3.226469080739142e-05, 'num_layers': 2, 'hidden_size': 126, 'dropout_prob': 0.3678089095301192}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224440-d6j2jm8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/d6j2jm8j' target=\"_blank\">stellar-field-294</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/d6j2jm8j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/d6j2jm8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 143 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "143 K     Trainable params\n",
      "0         Non-trainable params\n",
      "143 K     Total params\n",
      "0.575     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▂▃▃▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁██▂▅▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▂▁▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00092</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-field-294</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/d6j2jm8j' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/d6j2jm8j</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224440-d6j2jm8j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:45:20,654] Trial 84 finished with value: 0.0016491180285811424 and parameters: {'learning_rate': 5.2294296785461005e-05, 'num_layers': 2, 'hidden_size': 123, 'dropout_prob': 0.3703486156545311}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224520-vbldd13k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vbldd13k' target=\"_blank\">polished-frog-295</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vbldd13k' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vbldd13k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 150 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "150 K     Trainable params\n",
      "0         Non-trainable params\n",
      "150 K     Total params\n",
      "0.602     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▂█▁▂▁▁▂▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▄█▇█▆▅▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁▂▁▃▁▁▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.00472</td></tr><tr><td>train_loss_step</td><td>0.00218</td></tr><tr><td>trainer/global_step</td><td>4170</td></tr><tr><td>val_loss</td><td>0.00167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-frog-295</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vbldd13k' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vbldd13k</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224520-vbldd13k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:46:10,766] Trial 85 finished with value: 0.0016665500588715076 and parameters: {'learning_rate': 0.00039514561160148393, 'num_layers': 2, 'hidden_size': 126, 'dropout_prob': 0.3319954327594103}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224610-6tox8tvl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6tox8tvl' target=\"_blank\">drawn-sunset-296</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6tox8tvl' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6tox8tvl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 139 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "139 K     Trainable params\n",
      "0         Non-trainable params\n",
      "139 K     Total params\n",
      "0.557     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▂▁▃▂▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁█▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00619</td></tr><tr><td>trainer/global_step</td><td>3197</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-sunset-296</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6tox8tvl' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/6tox8tvl</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224610-6tox8tvl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:46:56,940] Trial 86 finished with value: 0.0016520594945177436 and parameters: {'learning_rate': 5.2709713402935386e-05, 'num_layers': 2, 'hidden_size': 121, 'dropout_prob': 0.3505733495222796}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224656-8wxrxbnx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8wxrxbnx' target=\"_blank\">gentle-blaze-297</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8wxrxbnx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8wxrxbnx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 145 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "145 K     Trainable params\n",
      "0         Non-trainable params\n",
      "145 K     Total params\n",
      "0.584     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▂▁▂▂▁▁▂▂▁▁▁▁▁▁▂▂▁▂▂▁▂▂▂▁▁▂▁▁▁▂█▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▄▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00092</td></tr><tr><td>trainer/global_step</td><td>3336</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-blaze-297</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8wxrxbnx' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/8wxrxbnx</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224656-8wxrxbnx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:47:42,819] Trial 87 finished with value: 0.001656201435253024 and parameters: {'learning_rate': 4.5983390654550196e-05, 'num_layers': 2, 'hidden_size': 124, 'dropout_prob': 0.3635227256482941}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224742-vq4xvsjl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vq4xvsjl' target=\"_blank\">super-disco-298</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vq4xvsjl' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vq4xvsjl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 148 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "148 K     Trainable params\n",
      "0         Non-trainable params\n",
      "148 K     Total params\n",
      "0.593     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▂▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▂▂▁▂▂▂▁▁▁▂▁▁▁▂█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00311</td></tr><tr><td>trainer/global_step</td><td>3058</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-disco-298</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vq4xvsjl' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/vq4xvsjl</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224742-vq4xvsjl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:48:27,105] Trial 88 finished with value: 0.001653387676924467 and parameters: {'learning_rate': 2.296896533509998e-05, 'num_layers': 2, 'hidden_size': 125, 'dropout_prob': 0.2816886596512489}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224827-4333l3lg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4333l3lg' target=\"_blank\">balmy-smoke-299</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4333l3lg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4333l3lg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 132 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.530     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▃▃▃▃▁▃█▁▅▁▃▂▁▃▃▃▆▄▁▄▃▂▃▁▃▂▂▂▆▁█▇▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>0.00487</td></tr><tr><td>train_loss_step</td><td>0.00223</td></tr><tr><td>trainer/global_step</td><td>1807</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-smoke-299</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4333l3lg' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/4333l3lg</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224827-4333l3lg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:49:05,031] Trial 89 finished with value: 0.0016579367220401764 and parameters: {'learning_rate': 0.00011943909628477692, 'num_layers': 2, 'hidden_size': 118, 'dropout_prob': 0.34492691674340564}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224905-jpw5hzhk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/jpw5hzhk' target=\"_blank\">amber-bee-300</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/jpw5hzhk' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/jpw5hzhk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 130 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "130 K     Trainable params\n",
      "0         Non-trainable params\n",
      "130 K     Total params\n",
      "0.521     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▂▃▃▂▄█▂▅▁▂▁▃▃▃▄▁▄▂▂▃▃▂▂▂▆█▇▂▅▂▄▁▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>0.00488</td></tr><tr><td>train_loss_step</td><td>0.00208</td></tr><tr><td>trainer/global_step</td><td>2363</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-bee-300</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/jpw5hzhk' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/jpw5hzhk</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224905-jpw5hzhk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:49:45,980] Trial 90 finished with value: 0.0016550779109820724 and parameters: {'learning_rate': 6.77578491181262e-05, 'num_layers': 2, 'hidden_size': 117, 'dropout_prob': 0.23411103451697146}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_224946-uxl9z7pv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uxl9z7pv' target=\"_blank\">icy-yogurt-301</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uxl9z7pv' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uxl9z7pv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 139 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "139 K     Trainable params\n",
      "0         Non-trainable params\n",
      "139 K     Total params\n",
      "0.557     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▃▂▁▁▁▁▂▂▁▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▁▂█▁▂▁▂▅▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00099</td></tr><tr><td>trainer/global_step</td><td>3753</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-yogurt-301</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uxl9z7pv' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/uxl9z7pv</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_224946-uxl9z7pv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:50:34,239] Trial 91 finished with value: 0.0016512623988091946 and parameters: {'learning_rate': 3.20801993723974e-05, 'num_layers': 2, 'hidden_size': 121, 'dropout_prob': 0.36945680559787686}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225034-msd5d2gn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/msd5d2gn' target=\"_blank\">solar-serenity-302</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/msd5d2gn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/msd5d2gn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 143 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "143 K     Trainable params\n",
      "0         Non-trainable params\n",
      "143 K     Total params\n",
      "0.575     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▂▁▂▁▂▁▁▁▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁█▁▂▁▂▂▂▂▁▁▁▁▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00411</td></tr><tr><td>trainer/global_step</td><td>4448</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-serenity-302</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/msd5d2gn' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/msd5d2gn</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225034-msd5d2gn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:51:24,370] Trial 92 finished with value: 0.00165120093151927 and parameters: {'learning_rate': 3.241261685935549e-05, 'num_layers': 2, 'hidden_size': 123, 'dropout_prob': 0.30577559102402196}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225124-wlgsk24g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wlgsk24g' target=\"_blank\">graceful-planet-303</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wlgsk24g' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wlgsk24g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 155 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "155 K     Trainable params\n",
      "0         Non-trainable params\n",
      "155 K     Total params\n",
      "0.621     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▂▁▆▃▃▁▃▁▅▄▂▃▃▆▄▁▂▂▁▃▂▂▁█▂▅▄▂▁▃▆▅▂▂▂▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>21</td></tr><tr><td>train_loss_epoch</td><td>0.00483</td></tr><tr><td>train_loss_step</td><td>0.00313</td></tr><tr><td>trainer/global_step</td><td>2919</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-planet-303</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wlgsk24g' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/wlgsk24g</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225124-wlgsk24g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:52:08,239] Trial 93 finished with value: 0.001653083716519177 and parameters: {'learning_rate': 2.58548849128744e-05, 'num_layers': 2, 'hidden_size': 128, 'dropout_prob': 0.35585444075440753}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225208-sbbwzoyz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/sbbwzoyz' target=\"_blank\">treasured-thunder-304</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/sbbwzoyz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/sbbwzoyz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 134 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁▁▂▁▁▂█▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▁▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00526</td></tr><tr><td>trainer/global_step</td><td>3614</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-thunder-304</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/sbbwzoyz' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/sbbwzoyz</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225208-sbbwzoyz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:52:56,193] Trial 94 finished with value: 0.0016522565856575966 and parameters: {'learning_rate': 1.723376135782469e-05, 'num_layers': 2, 'hidden_size': 119, 'dropout_prob': 0.32645311741622285}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225256-5mgxy1nf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5mgxy1nf' target=\"_blank\">eager-sponge-305</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5mgxy1nf' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5mgxy1nf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 123 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "123 K     Trainable params\n",
      "0         Non-trainable params\n",
      "123 K     Total params\n",
      "0.496     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▂▁▁▁▁▁▁▂▂▁▁▁▂▁▂▁▂▂▁▂▁▁█▁▂▂▅▂▂▁▁▂▁▁▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▆▅█▆▅▅▅▄▄▃▃▂▄▃▂▂▂▂▂▂▃▃▂▁▃▂▂▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.0048</td></tr><tr><td>train_loss_step</td><td>0.00179</td></tr><tr><td>trainer/global_step</td><td>5004</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sponge-305</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5mgxy1nf' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/5mgxy1nf</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225256-5mgxy1nf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:53:49,479] Trial 95 finished with value: 0.0016595034394413233 and parameters: {'learning_rate': 0.0001636256539622634, 'num_layers': 2, 'hidden_size': 114, 'dropout_prob': 0.3949014162745044}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225349-7l9m9z7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7l9m9z7f' target=\"_blank\">unique-durian-306</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7l9m9z7f' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7l9m9z7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 141 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.566     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁▁▂▁▁▂█▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00536</td></tr><tr><td>trainer/global_step</td><td>3614</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-durian-306</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7l9m9z7f' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/7l9m9z7f</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225349-7l9m9z7f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:54:37,924] Trial 96 finished with value: 0.0016534203896299005 and parameters: {'learning_rate': 3.9462628801490615e-05, 'num_layers': 2, 'hidden_size': 122, 'dropout_prob': 0.3622340264610687}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225437-y3puoaum</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/y3puoaum' target=\"_blank\">gallant-pond-307</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/y3puoaum' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/y3puoaum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 117 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▂▂▁▁▂▁▁▂▁▁▂▁▁▂█▁▂▁▂▅▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>train_loss_epoch</td><td>0.00482</td></tr><tr><td>train_loss_step</td><td>0.00365</td></tr><tr><td>trainer/global_step</td><td>3892</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-pond-307</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/y3puoaum' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/y3puoaum</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225437-y3puoaum\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:55:26,866] Trial 97 finished with value: 0.0016558751231059432 and parameters: {'learning_rate': 5.70926415143354e-05, 'num_layers': 2, 'hidden_size': 111, 'dropout_prob': 0.37125914739493887}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225526-x6i5ik0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x6i5ik0f' target=\"_blank\">cool-forest-308</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x6i5ik0f' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x6i5ik0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 148 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "148 K     Trainable params\n",
      "0         Non-trainable params\n",
      "148 K     Total params\n",
      "0.593     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▇▃▃▃▁█▂▅▄▂▁▃▃▆▄▄▃▂▁▃▂▂▆▁█▂▅▂▃▁▄▆▅▆▃▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▃▃▁▁▁▁▂▂▂▂▂▂▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>0.00484</td></tr><tr><td>train_loss_step</td><td>0.00279</td></tr><tr><td>trainer/global_step</td><td>2641</td></tr><tr><td>val_loss</td><td>0.00166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-forest-308</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x6i5ik0f' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/x6i5ik0f</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225526-x6i5ik0f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:56:08,772] Trial 98 finished with value: 0.001657161395996809 and parameters: {'learning_rate': 9.796621474096517e-05, 'num_layers': 2, 'hidden_size': 125, 'dropout_prob': 0.3848602058727114}. Best is trial 29 with value: 0.0016484229126945138.\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\janfr\\AppData\\Local\\Temp\\ipykernel_14440\\945958637.py:55: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janfr\\Documents\\python_projects\\masterarbeit\\NN_model\\wandb\\run-20240502_225608-tj3xvfml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tj3xvfml' target=\"_blank\">leafy-breeze-309</a></strong> to <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tj3xvfml' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tj3xvfml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\janfr\\anaconda3\\envs\\pytorch_3_10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | GRU     | 130 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "130 K     Trainable params\n",
      "0         Non-trainable params\n",
      "130 K     Total params\n",
      "0.521     Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▂▁▂▂▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▁▁▂▁▁▂▁▁▂█▁▂▁▂▅▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_loss_epoch</td><td>0.00489</td></tr><tr><td>train_loss_step</td><td>0.00122</td></tr><tr><td>trainer/global_step</td><td>4031</td></tr><tr><td>val_loss</td><td>0.00165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-breeze-309</strong> at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tj3xvfml' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning/runs/tj3xvfml</a><br/> View project at: <a href='https://wandb.ai/frederik135/optuna_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/frederik135/optuna_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240502_225608-tj3xvfml\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 22:56:58,882] Trial 99 finished with value: 0.001653599552810192 and parameters: {'learning_rate': 2.128933509463325e-05, 'num_layers': 2, 'hidden_size': 117, 'dropout_prob': 0.34206600855197405}. Best is trial 29 with value: 0.0016484229126945138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.00018887149084660608, 'num_layers': 2, 'hidden_size': 37, 'dropout_prob': 0.45255852589403345}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "from models import LSTM, GRU, FCNN\n",
    "from config import model_config, device, seq_length, architecture\n",
    "from preprocessing import train_loader, val_loader, label_scaler\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader, test_dates):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.test_dates = test_dates\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    dropout_prob = trial.suggest_uniform('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    # Update model configuration\n",
    "    model_config.update({\n",
    "        \"hidden_layer_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_prob\": dropout_prob\n",
    "    })\n",
    "\n",
    "    # Update wandb configuration for the current trial\n",
    "    wandb_config = {\n",
    "        \"architecture\": architecture,\n",
    "        \"num_units\": hidden_size,  # Reflects the current trial's suggestion\n",
    "        \"num_layers\": num_layers,  # Reflects the current trial's suggestion\n",
    "        \"dropout\": dropout_prob,  # Reflects the current trial's suggestion\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50,  # Reflect current setup\n",
    "        \"learning_rate\": learning_rate  # Reflects the current trial's suggestion\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", reinit=True, config=wandb_config)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler,\n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None, test_dates=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    # Device agnostic initialization\n",
    "    if torch.cuda.is_available():\n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif hasattr(torch, 'has_mps') and torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"optuna_hyperparameter_tuning\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
