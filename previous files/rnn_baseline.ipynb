{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, ob GPU vorhanden ist\n",
    "\n",
    "import torch\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUs anzeigen lassen\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meeting mit Benjamin, 25.03.\n",
    "# Output als tatsächliche Preise schreiben\n",
    "# So das Programm schreiben, dass man die Modelle austauschen kann (RNN; LSTM; GRU)\n",
    "\n",
    "# Weights speichern und danach wiederverwenden\n",
    "# Pytorch Lightning verwenden\n",
    "# Preprocessing speziell für Time Series (Differencing und Log) -> Data normalization\n",
    "# Welche zusätzliche Daten (sin, cos encoding vom Tag, Woche, Monat) - Relevant für Seasonalität\n",
    "# Sin cos encoding\n",
    "# Mögliche Modelle: State Space Models (Mamba)\n",
    "# Vor 2022 als training set benutzen und 2022 (neues Regime) als validation um Model zu testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with tensorboard\n",
    "After running the script, open terminal, cd to project directory and \\\n",
    "`tensorboard --logdir=runs` \\\n",
    "\\\n",
    "In your browser, open \\\n",
    "[http://localhost:6006/](http://localhost:6006/)  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from configuration import stock_ticker, company_name, architecture, num_units, num_layers, dropout_prob, model, seq_length, start_date, end_date, num_epochs, learning_rate\n",
    "import wandb\n",
    "import os\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        xs.append(data[i:(i+seq_length)])\n",
    "        ys.append(data[i+seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def train_and_validate_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for seqs, labels in train_loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seqs)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        training_loss = np.mean(train_losses)\n",
    "        training_losses.append(training_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs, labels = seqs.to(device), labels.to(device)\n",
    "                y_pred = model(seqs)\n",
    "                val_losses.append(criterion(y_pred, labels).item())\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        wandb.log({\"training_loss\": training_loss, \"validation_loss\": validation_loss})\n",
    "\n",
    "        \"\"\"\n",
    "        if writer:\n",
    "            writer.add_scalar('Loss/train', training_loss, epoch)\n",
    "            writer.add_scalar('Loss/validation', validation_loss, epoch)\n",
    "        \"\"\"\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch} | Training Loss: {training_loss} | Validation Loss: {validation_loss}')\n",
    "    \n",
    "    return training_losses, validation_losses\n",
    "\n",
    "def plot_losses(training_losses, validation_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels in test_loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            output = model(seqs)\n",
    "            predictions.extend(output.view(-1).tolist())\n",
    "            actuals.extend(labels.view(-1).tolist())\n",
    "    return np.array(predictions), np.array(actuals)\n",
    "\n",
    "def inverse_transform_scaler(values, scaler):\n",
    "    values_reshaped = values.reshape(-1, 1)\n",
    "    return scaler.inverse_transform(values_reshaped)\n",
    "\n",
    "\"\"\"\n",
    "def plot_predictions_with_dates(predictions, actuals, dates, scaler):\n",
    "    predictions_original_scale = inverse_transform_scaler(np.array(predictions), scaler)\n",
    "    actuals_original_scale = inverse_transform_scaler(np.array(actuals), scaler)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(dates, actuals_original_scale, label='Actual Price', color='blue', marker='o')\n",
    "    plt.plot(dates, predictions_original_scale, label='Predicted Price', color='red', linestyle='--')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"label\": plt})\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "def plot_predictions_with_dates(predictions, actuals, dates, scaler):\n",
    "    predictions_original_scale = list(inverse_transform_scaler(np.array(predictions), scaler))\n",
    "    actuals_original_scale = list(inverse_transform_scaler(np.array(actuals), scaler))\n",
    "    baseline_original_scale = [actuals_original_scale[0]] + actuals_original_scale[:-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    ax.plot(dates, actuals_original_scale, label='Actual Price', color='black', linestyle='-', marker='o')\n",
    "    ax.plot(dates, predictions_original_scale, label='Predicted Price', color='green', linestyle='-')\n",
    "    ax.plot(dates, baseline_original_scale, label='Baseline', color='blue', linestyle='-', marker='o')\n",
    "\n",
    "    ax.set_title('Stock Price Prediction')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Stock Price')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    filename = \"plot.png\"\n",
    "    fig.savefig(filename)\n",
    "    wandb.log({\"Stock Price Prediction\": wandb.Image(filename)})\n",
    "    os.remove(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"RNN_single_step_forecasts\",\n",
    "    config={\n",
    "        \"dataset\": f\"{company_name} closing prices\",\n",
    "        \"architecture\": architecture,\n",
    "        \"num_units\": num_units,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# writer = SummaryWriter('runs/rnn_model')\n",
    "\n",
    "# Obtaining data and preprocessing\n",
    "stock_df = yf.download(stock_ticker, start=start_date, end=end_date)\n",
    "\n",
    "split_idx = int(len(stock_df) * 0.8) + seq_length\n",
    "train_df = stock_df.iloc[:split_idx]\n",
    "temp_df = stock_df.iloc[split_idx:]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_normalized = scaler.fit_transform(train_df[['Close']].values.reshape(-1, 1))\n",
    "X_train, y_train = create_sequences(train_normalized, seq_length)\n",
    "\n",
    "temp_normalized = scaler.transform(temp_df[['Close']].values.reshape(-1, 1))\n",
    "X_temp, y_temp = create_sequences(temp_normalized, seq_length)\n",
    "\n",
    "split_idx_temp = len(y_temp) // 2\n",
    "X_val, X_test = X_temp[:split_idx_temp], X_temp[split_idx_temp:]\n",
    "y_val, y_test = y_temp[:split_idx_temp], y_temp[split_idx_temp:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)), batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)), batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test)), batch_size=64, shuffle=False)\n",
    "\n",
    "# Add model graph to tensorboard\n",
    "# sample_input_tensor = torch.randn((1, seq_length, 1)).to(device)\n",
    "# writer.add_graph(model, sample_input_tensor)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation\n",
    "training_losses, validation_losses = train_and_validate_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "plot_losses(training_losses, validation_losses)\n",
    "\n",
    "# Evaluation of the model on the test set\n",
    "predictions, actuals = evaluate_model(model, test_loader)\n",
    "\n",
    "print(predictions, actuals)\n",
    "\n",
    "dates = list(stock_df.index[(-len(predictions)):])\n",
    "dates = sorted(dates)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(actuals, predictions)}')\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(actuals, predictions))}')\n",
    "print(f'MAE: {mean_absolute_error(actuals, predictions)}')\n",
    "print(f'R-squared: {r2_score(actuals, predictions)}')\n",
    "print(f'MAPE: {np.mean(np.abs(actuals - predictions) / actuals)}')\n",
    "\n",
    "plot_predictions_with_dates(predictions, actuals, dates, scaler)\n",
    "wandb.finish()\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from configuration import stock_ticker, start_date, end_date\n",
    "\n",
    "stock_df = yf.download(stock_ticker, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['baseline'] = stock_df['Close'].shift(1)\n",
    "stock_df['baseline'] = stock_df['baseline'].fillna(method='bfill')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrederik135\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/wandb/run-20240414_105520-g1vkofll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/RNN_single_step_forecasts/runs/g1vkofll' target=\"_blank\">stellar-wind-62</a></strong> to <a href='https://wandb.ai/frederik135/RNN_single_step_forecasts' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/RNN_single_step_forecasts' target=\"_blank\">https://wandb.ai/frederik135/RNN_single_step_forecasts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/RNN_single_step_forecasts/runs/g1vkofll' target=\"_blank\">https://wandb.ai/frederik135/RNN_single_step_forecasts/runs/g1vkofll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['baseline'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m stock_df\u001b[38;5;241m.\u001b[39miloc[split_idx:]\n\u001b[1;32m     38\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 39\u001b[0m train_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     40\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m create_sequences(train_normalized, seq_length)\n\u001b[1;32m     42\u001b[0m temp_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(temp_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['baseline'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from configuration import stock_ticker, company_name, architecture, num_units, num_layers, dropout_prob, model, seq_length, start_date, end_date, num_epochs, learning_rate\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"RNN_single_step_forecasts\",\n",
    "    config={\n",
    "        \"dataset\": f\"{company_name} baseline\",\n",
    "        \"architecture\": architecture,\n",
    "        \"num_units\": num_units,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# writer = SummaryWriter('runs/rnn_model')\n",
    "\n",
    "stock_df = yf.download(stock_ticker, start=start_date, end=end_date)\n",
    "\n",
    "split_idx = int(len(stock_df) * 0.8) + seq_length\n",
    "train_df = stock_df.iloc[:split_idx]\n",
    "temp_df = stock_df.iloc[split_idx:]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_normalized = scaler.fit_transform(train_df[['baseline']].values.reshape(-1, 1))\n",
    "X_train, y_train = create_sequences(train_normalized, seq_length)\n",
    "\n",
    "temp_normalized = scaler.transform(temp_df[['baseline']].values.reshape(-1, 1))\n",
    "X_temp, y_temp = create_sequences(temp_normalized, seq_length)\n",
    "\n",
    "split_idx_temp = len(y_temp) // 2\n",
    "X_val, X_test = X_temp[:split_idx_temp], X_temp[split_idx_temp:]\n",
    "y_val, y_test = y_temp[:split_idx_temp], y_temp[split_idx_temp:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)), batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)), batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test)), batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "training_losses, validation_losses = train_and_validate_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "plot_losses(training_losses, validation_losses)\n",
    "\n",
    "predictions, actuals = evaluate_model(model, test_loader)\n",
    "dates = list(stock_df.index[(-len(predictions)):])\n",
    "dates = sorted(dates)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(actuals, predictions)}')\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(actuals, predictions))}')\n",
    "print(f'MAE: {mean_absolute_error(actuals, predictions)}')\n",
    "print(f'R-squared: {r2_score(actuals, predictions)}')\n",
    "print(f'MAPE: {np.mean(np.abs(actuals - predictions) / actuals)}')\n",
    "\n",
    "plot_predictions_with_dates(predictions, actuals, dates, scaler)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
