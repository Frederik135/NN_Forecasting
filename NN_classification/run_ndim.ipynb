{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from config import num_epochs, learning_rate, wandb_config, model\n",
    "from preprocessing import train_loader, val_loader, test_loader, test_dates\n",
    "\n",
    "class StockPredictionModule(pl.LightningModule):\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, test_dates):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        labels = labels.long()  # Ensure labels are long integers\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        labels = labels.long()  # Ensure labels are long integers\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        labels = labels.long()  # Ensure labels are long integers\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "def on_test_epoch_end(self):\n",
    "    # Prepare metric calculators\n",
    "    accuracy = torchmetrics.Accuracy()\n",
    "    precision = torchmetrics.Precision(num_classes=3, average='macro')\n",
    "    recall = torchmetrics.Recall(num_classes=3, average='macro')\n",
    "    f1 = torchmetrics.F1(num_classes=3, average='macro')\n",
    "\n",
    "    predictions, actuals = [], []\n",
    "    for seqs, labels in self.test_loader:\n",
    "        seqs, labels = seqs.to(self.device), labels.to(self.device)\n",
    "        logits = self(seqs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.view(-1).cpu().numpy())\n",
    "        actuals.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "        # Update metric calculations\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "    # Compute the final metrics for this epoch\n",
    "    final_accuracy = accuracy.compute()\n",
    "    final_precision = precision.compute()\n",
    "    final_recall = recall.compute()\n",
    "    final_f1 = f1.compute()\n",
    "\n",
    "    # Log metrics to Weights & Biases\n",
    "    wandb.log({\n",
    "        \"test_accuracy\": final_accuracy,\n",
    "        \"test_precision\": final_precision,\n",
    "        \"test_recall\": final_recall,\n",
    "        \"test_f1\": final_f1\n",
    "    })\n",
    "\n",
    "    # Clear memory\n",
    "    accuracy.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1.reset()\n",
    "\n",
    "    # Optional: Visualize results with matplotlib and log plots to wandb\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(actuals, label='Actuals', marker='o')\n",
    "    ax.plot(predictions, label='Predictions', marker='x')\n",
    "    ax.set_title('Actual vs Predicted Labels')\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Class Label')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    fig.savefig(\"predictions_vs_actuals.png\")\n",
    "    wandb.log({\"Predictions vs Actuals\": wandb.Image(\"predictions_vs_actuals.png\")})\n",
    "    os.remove(\"predictions_vs_actuals.png\")\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    wandb_logger = WandbLogger(project=\"LSTM_classification_single_step\", log_model=\"all\", config=wandb_config)\n",
    "    \n",
    "    module = StockPredictionModule(model=model, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, test_dates = test_dates)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif hasattr(torch, 'has_mps') and torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    trainer = Trainer(max_epochs=num_epochs, logger=wandb_logger, accelerator=accelerator, devices=devices, enable_checkpointing=True)\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(dataloaders=test_loader, ckpt_path=\"best\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Optimization (Objective: Minimize validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "from models import LSTM, GRU, FCNN\n",
    "from config import model_config, device, seq_length, architecture\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label - 1 for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction - 1 for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_layer_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_prob\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": architecture,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    accelerator = \"auto\"\n",
    "    devices = 1 if torch.cuda.is_available() or torch.backends.mps.is_built() else None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"optuna_hyperparameter_tuning\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (Objective: Maximize R2 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "from models import LSTM, GRU, FCNN\n",
    "from config import model_config, device, seq_length, architecture\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        r2 = r2_score(labels, y_pred)\n",
    "        mse = mean_squared_error(labels, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels, y_pred)\n",
    "        mape = np.mean(np.abs((labels - y_pred) / (y_pred + 1e-8)))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape}\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_layer_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_prob\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    accelerator = \"auto\"\n",
    "    devices = 1 if torch.cuda.is_available() or torch.backends.mps.is_built() else None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"optuna_hyperparameter_tuning\", log_model=\"all\")\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    val_r2 = val_result[0].get('val_r2', float('-inf')) \n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return -np.exp(val_r2 + 1.0)\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization for FCNN (Objective: Minimize validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from models import FCNN_model\n",
    "from config import device, seq_length, num_features\n",
    "from preprocessing import train_loader, val_loader, label_scaler\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label - 1 for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction - 1 for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_size_{i}', 32, 128) for i in range(num_hidden_layers)]\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config = {\n",
    "        \"seq_length\": seq_length,\n",
    "        \"num_features\": num_features,\n",
    "        \"hidden_layers\": hidden_layers,\n",
    "        \"n_out\": 1, \n",
    "        \"dropout_prob\": dropout_prob\n",
    "    }\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": \"FCNN\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"hidden_layers\": hidden_layers,\n",
    "        \"dropout_prob\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"fcnn_hyperparameter_test\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "\n",
    "    model = FCNN_model(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif hasattr(torch, 'has_mps') and torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"fcnn_hyperparameter_test\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
