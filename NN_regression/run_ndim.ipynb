{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from config import num_epochs, learning_rate, wandb_config, model\n",
    "# from preprocessing import stock_df\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "\n",
    "class StockPredictionModule(pl.LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # self.criterion = torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        predictions, actuals = [], []\n",
    "        for seqs, labels in self.test_loader:\n",
    "            seqs, labels = seqs.to(self.device), labels.to(self.device)\n",
    "            output = self(seqs)\n",
    "            predictions.extend(output.view(-1).detach().cpu().numpy())\n",
    "            actuals.extend(labels.view(-1).detach().cpu().numpy())\n",
    "\n",
    "        predictions_rescaled = list(self.label_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten())\n",
    "        actuals_rescaled = list(self.label_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten())\n",
    "        baseline_rescaled = [actuals_rescaled[0]] + actuals_rescaled[:-1]\n",
    "        baseline_constant = [0.0] * len(predictions_rescaled)\n",
    "\n",
    "        len_test_set = len(predictions)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(actuals_rescaled[-100:], label='Actual log returns', color='black', linestyle='-')\n",
    "        ax.plot(predictions_rescaled[-100:], label='Predicted log returns', color='green', linestyle='-')\n",
    "        # ax.plot(baseline_rescaled[-100:], label='Baseline_1', color='darkblue', linestyle='-')\n",
    "        # ax.plot(baseline_constant[-100:], label='Baseline_2', color='steelblue', linestyle='-')\n",
    "        # ax.plot(test_dates[-100:], arima_predictions[-100:], label='Baseline', color='orange', linestyle='-') \n",
    "        ax.set_title('Log returns prediction')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Log returns')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Relative Difference Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Use plotting with rebasing to visualize the predictions as prices\n",
    "        rebase_period = 30\n",
    "        predicted_prices = [actual_closing_prices[0]]\n",
    "        for i, relative_change in enumerate(predictions_rescaled[1:], 1):\n",
    "            if i % rebase_period == 0:\n",
    "                predicted_prices.append(actual_closing_prices[i])\n",
    "            else:\n",
    "                predicted_prices.append(predicted_prices[-1] * relative_change)\n",
    "        baseline_prices = [actual_closing_prices[0]] + list(actual_closing_prices[:-1])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(test_dates, actual_closing_prices, label='Actual Price', color='black', linestyle='-')\n",
    "        ax.plot(test_dates, predicted_prices, label='Predicted Price', color='green', linestyle='-')\n",
    "        ax.plot(test_dates, baseline_prices, label='Baseline', color='blue', linestyle='-') \n",
    "        ax.set_title('Price predictions based on last price in validation set')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Stock Price Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # We use for the first value the actual closing price and then multiply the relative change for each following timestep\n",
    "        actual_prices = [actual_closing_prices[0]]\n",
    "        for i in range(1, len(actuals_rescaled)):\n",
    "            actual_prices.append(actual_prices[i-1] * actuals_rescaled[i])\n",
    "        prediction_prices = [actual_closing_prices[0]]\n",
    "        for i in range(1, len(predictions_rescaled)):\n",
    "            prediction_prices.append(prediction_prices[i-1] * predictions_rescaled[i])\n",
    "        baseline_prices = [actual_prices[0]] + actual_prices[:-1]\n",
    "        fig, ax = plt.subplots(figsize=(15, 7))\n",
    "        ax.plot(test_dates, actual_prices, label='Actual Price', color='black', linestyle='-')\n",
    "        ax.plot(test_dates, prediction_prices, label='Predicted Price', color='green', linestyle='-')\n",
    "        ax.plot(test_dates, baseline_prices, label='Baseline', color='blue', linestyle='-')\n",
    "        ax.set_title('Stock Price Prediction')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.legend()\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        filename = \"plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Stock Price Prediction\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "        \"\"\"\n",
    "        \n",
    "        net_abs_dev = torch.tensor([abs(predictions_rescaled[i] - actuals_rescaled[i]) for i in range(len(actuals_rescaled))])\n",
    "        baseline_abs_dev = torch.tensor([abs(baseline_rescaled[i] - actuals_rescaled[i]) for i in range(len(actuals_rescaled))])\n",
    "        diff_pos = torch.relu(baseline_abs_dev - net_abs_dev).reshape(-1).tolist()\n",
    "        diff_min = (-torch.relu(net_abs_dev - baseline_abs_dev)).reshape(-1).tolist()\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Model vs baseline performance comparison on test samples')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), diff_pos, color='g', label='Model Wins', width=1.0)\n",
    "        ax.bar(list(range(len(actuals_rescaled))), diff_min, color='r', label='Baseline Wins', width=1.0)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Difference in Absolute Deviation')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Model vs Baseline Performance Comparison\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        model_actual_dev = torch.tensor([predictions_rescaled[i] - actuals_rescaled[i] for i in range(len(actuals_rescaled))])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Model deviations from actuals')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), model_actual_dev, color='g', label='Model Wins', width=1.0)\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Deviation from actuals')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Model deviations from actuals\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        baseline_actual_dev = torch.tensor([baseline_rescaled[i] - actuals_rescaled[i] for i in range(len(actuals_rescaled))])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title('Baseline deviations from actuals')\n",
    "        ax.hlines(0, xmin=0, xmax=len(actuals_rescaled), linestyles='dashed', colors='black')\n",
    "        ax.bar(list(range(len(actuals_rescaled))), baseline_actual_dev, color='b', label='Baseline Wins', width=1.0)\n",
    "        ax.set_xlabel('Test Sample Index')\n",
    "        ax.set_ylabel('Deviation from actuals')\n",
    "        plt.show()\n",
    "        filename = \"comparison_plot.png\"\n",
    "        fig.savefig(filename)\n",
    "        wandb.log({\"Baseline deviations from actuals\": wandb.Image(filename)})\n",
    "        os.remove(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "        actuals_rescaled = np.array(actuals_rescaled)\n",
    "        predictions_rescaled = np.array(predictions_rescaled)\n",
    "        baseline_rescaled = np.array(baseline_rescaled)\n",
    "\n",
    "        model_mse = mean_squared_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_rmse = np.sqrt(model_mse)\n",
    "        model_mae = mean_absolute_error(actuals_rescaled, predictions_rescaled)\n",
    "        model_r2 = r2_score(actuals_rescaled, predictions_rescaled)\n",
    "        model_mape = np.mean(np.abs((actuals_rescaled - predictions_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label for label in actuals_rescaled]\n",
    "        pct_change_predictions = [pred for pred in predictions_rescaled]\n",
    "        hit_rate_model = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "        \n",
    "        baseline_mse = mean_squared_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_rmse = np.sqrt(baseline_mse)\n",
    "        baseline_mae = mean_absolute_error(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_r2 = r2_score(actuals_rescaled, baseline_rescaled)\n",
    "        baseline_mape = np.mean(np.abs((actuals_rescaled - baseline_rescaled) / (actuals_rescaled + 1e-8)))\n",
    "        pct_change_baseline = [base for base in baseline_rescaled]\n",
    "        hit_rate_baseline = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_baseline))\n",
    "\n",
    "        model_metrics = {\n",
    "            \"mse\": model_mse,\n",
    "            \"rmse\": model_rmse,\n",
    "            \"mae\": model_mae,\n",
    "            \"mape\": model_mape,\n",
    "            \"r2\": model_r2,\n",
    "            \"hit_rate\": hit_rate_model,\n",
    "        }\n",
    "        baseline_metrics = {\n",
    "            \"mse\": baseline_mse,\n",
    "            \"rmse\": baseline_rmse,\n",
    "            \"mae\": baseline_mae,\n",
    "            \"mape\": baseline_mape,\n",
    "            \"r2\": baseline_r2,\n",
    "            \"hit_rate\": hit_rate_baseline\n",
    "        }\n",
    "        model_baseline_performance_metrics = {\n",
    "            \"mse\": round((baseline_mse / model_mse - 1) * 100, 2),\n",
    "            \"rmse\": round((baseline_rmse / model_rmse - 1) * 100, 2),\n",
    "            \"mae\": round((baseline_mae / model_mae - 1) * 100, 2),\n",
    "            \"mape\": round((baseline_mape / model_mape - 1) * 100, 2),\n",
    "            \"r2\": round((model_r2 / baseline_r2 - 1) * 100, 2),\n",
    "            \"hit_rate\": round((hit_rate_model / hit_rate_baseline - 1) * 100, 2),\n",
    "        }\n",
    "\n",
    "        print(\"Preparing to log the table...\")\n",
    "        metrics_table = wandb.Table(columns=[\"metric\", \"model\", \"baseline\", \"model-baseline performance comparison [%]\"])\n",
    "        for metric in model_metrics.keys():\n",
    "            metrics_table.add_data(metric, model_metrics[metric], baseline_metrics[metric], model_baseline_performance_metrics[metric])\n",
    "        wandb.log({\"metrics\": metrics_table})\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    wandb_logger = WandbLogger(project=\"RNN_single_step_forecasts\", log_model=\"all\", config=wandb_config)\n",
    "    \n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    trainer = Trainer(max_epochs=num_epochs, logger=wandb_logger, accelerator=accelerator, devices=devices, enable_checkpointing=True)\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(dataloaders=test_loader, ckpt_path=\"best\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Optimization (Objective: Minimize validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "from models import LSTM, GRU, FCNN\n",
    "from config import model_config, device, seq_length, architecture\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label - 1 for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction - 1 for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_layer_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_prob\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": architecture,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    accelerator = \"auto\"\n",
    "    devices = 1 if torch.cuda.is_available() or torch.backends.mps.is_built() else None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"optuna_hyperparameter_tuning\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (Objective: Maximize R2 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "from models import LSTM, GRU, FCNN\n",
    "from config import model_config, device, seq_length, architecture\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        r2 = r2_score(labels, y_pred)\n",
    "        mse = mean_squared_error(labels, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels, y_pred)\n",
    "        mape = np.mean(np.abs((labels - y_pred) / (y_pred + 1e-8)))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape}\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_layer_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_prob\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"optuna_hyperparameter_tuning\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = GRU(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    accelerator = \"auto\"\n",
    "    devices = 1 if torch.cuda.is_available() or torch.backends.mps.is_built() else None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"optuna_hyperparameter_tuning\", log_model=\"all\")\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    val_r2 = val_result[0].get('val_r2', float('-inf')) \n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return -np.exp(val_r2 + 1.0)\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization for FCNN (Objective: Minimize validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from models import FCNN_model\n",
    "from config import device, seq_length, num_features\n",
    "from preprocessing import train_loader, val_loader, label_scaler\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label - 1 for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction - 1 for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_size_{i}', 32, 128) for i in range(num_hidden_layers)]\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    model_config = {\n",
    "        \"seq_length\": seq_length,\n",
    "        \"num_features\": num_features,\n",
    "        \"hidden_layers\": hidden_layers,\n",
    "        \"n_out\": 1, \n",
    "        \"dropout_prob\": dropout_prob\n",
    "    }\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": \"FCNN\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"hidden_layers\": hidden_layers,\n",
    "        \"dropout_prob\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"fcnn_hyperparameter_test\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "\n",
    "    model = FCNN_model(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif hasattr(torch, 'has_mps') and torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"fcnn_hyperparameter_test\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[I 2024-06-02 14:00:01,222] A new study created in memory with name: no-name-7be9d591-fe5a-46f0-b75b-f6ea1055510f\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrederik135\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140002-h8f6ymc5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/h8f6ymc5' target=\"_blank\">snowy-bee-6</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/h8f6ymc5' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/h8f6ymc5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 790 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "790 K     Trainable params\n",
      "0         Non-trainable params\n",
      "790 K     Total params\n",
      "3.160     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2e841936034db68de645b9d6edff4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▆▃▂▂▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▆▆▅▃▃▃▂▂▃▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mape</td><td>█▅▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁▅▆▆▇▇██████████████████████████████████</td></tr><tr><td>val_rmse</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00266</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.00172</td></tr><tr><td>val_mae</td><td>0.00657</td></tr><tr><td>val_mape</td><td>0.00655</td></tr><tr><td>val_mse</td><td>8e-05</td></tr><tr><td>val_r2</td><td>-0.04663</td></tr><tr><td>val_rmse</td><td>0.0089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-bee-6</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/h8f6ymc5' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/h8f6ymc5</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140002-h8f6ymc5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:01:47,580] Trial 0 finished with value: 0.0017164145829156041 and parameters: {'learning_rate': 1.0011081325003182e-05, 'num_layers': 2, 'num_heads': 8, 'hidden_size_multipliers': 11, 'dropout_prob': 0.24393125530713555}. Best is trial 0 with value: 0.0017164145829156041.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58d59cd5f1a441db6d0391905ea7cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011154017588890182, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140147-08c98dcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/08c98dcz' target=\"_blank\">floral-cloud-7</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/08c98dcz' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/08c98dcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 30.9 K\n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "30.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.9 K    Total params\n",
      "0.123     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585f1713236146b79068da3b943ed372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.1498723714598274, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mape</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁█████████████</td></tr><tr><td>val_rmse</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00215</td></tr><tr><td>trainer/global_step</td><td>1807</td></tr><tr><td>val_loss</td><td>0.00166</td></tr><tr><td>val_mae</td><td>0.00642</td></tr><tr><td>val_mape</td><td>0.00641</td></tr><tr><td>val_mse</td><td>8e-05</td></tr><tr><td>val_r2</td><td>-0.00904</td></tr><tr><td>val_rmse</td><td>0.00874</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-cloud-7</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/08c98dcz' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/08c98dcz</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140147-08c98dcz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:02:43,509] Trial 1 finished with value: 0.0016582789830863476 and parameters: {'learning_rate': 0.003018704416886663, 'num_layers': 3, 'num_heads': 2, 'hidden_size_multipliers': 1, 'dropout_prob': 0.12764197639858893}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cd5243a19046d892bc9cf18119cfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011126127777778392, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140243-8tkvukjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8tkvukjf' target=\"_blank\">frosty-plasma-8</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8tkvukjf' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8tkvukjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 55.7 K\n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "55.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.7 K    Total params\n",
      "0.223     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b6670e30044dcba5b93e64c719c85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▄▃▂▇▄▂▅▂▃▂▆▂▂▅▄▂▂▁▆▃▁▅▃▃▁▂▁▂▃█▁▁▂▇▃▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▆▄▆██▇▃▅▅▃▃▂▃▂▃▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>val_mae</td><td>▆▄▆██▇▃▅▅▂▂▂▃▂▃▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_mape</td><td>▆▄▆██▇▃▅▅▂▂▂▃▂▃▂▂▂▂▂▁▂▁▁▂▁▁▂▃▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>▆▄▆██▇▃▅▅▃▃▂▃▂▃▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>val_r2</td><td>▃▅▃▁▁▂▆▄▄▇▆▇▆▇▆▇▇▇▇▇█▇██▇██▇▆▇████▇█████</td></tr><tr><td>val_rmse</td><td>▆▄▇██▇▃▅▅▃▃▂▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▁▁▁▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00145</td></tr><tr><td>trainer/global_step</td><td>6533</td></tr><tr><td>val_loss</td><td>0.0017</td></tr><tr><td>val_mae</td><td>0.00653</td></tr><tr><td>val_mape</td><td>0.00651</td></tr><tr><td>val_mse</td><td>8e-05</td></tr><tr><td>val_r2</td><td>-0.0413</td></tr><tr><td>val_rmse</td><td>0.00887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-plasma-8</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8tkvukjf' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8tkvukjf</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140243-8tkvukjf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:04:59,966] Trial 2 finished with value: 0.0017043797997757792 and parameters: {'learning_rate': 0.003057018011582537, 'num_layers': 3, 'num_heads': 2, 'hidden_size_multipliers': 2, 'dropout_prob': 0.27092519524035186}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce664891e354940be7ccede7cdb0f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011158434266667428, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140459-awbn84uf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/awbn84uf' target=\"_blank\">genial-resonance-9</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/awbn84uf' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/awbn84uf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 526 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "526 K     Trainable params\n",
      "0         Non-trainable params\n",
      "526 K     Total params\n",
      "2.106     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2507dc211e478ca55f6baa3e7e9466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▇█▃▂▂▃▂▂▂▂▂▁▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>▃▂▂▂▂▂▁▁▁▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁█▁▂▂▂▂▂▂▁▁▁▁▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▂▂▃▁▁▁▁▁▃▂▂▂▁▁▃▁▁▁▁▁▃▁▂▁▂▂▃█▂▃▃</td></tr><tr><td>val_mae</td><td>█▆▃▃▃▂▂▂▁▂▃▂▂▂▂▁▃▁▁▁▁▁▃▁▂▁▂▂▂▆▂▂▂</td></tr><tr><td>val_mape</td><td>█▅▃▃▃▂▂▂▁▂▂▂▂▂▂▁▃▁▁▁▁▁▃▁▂▁▂▂▂▆▁▂▂</td></tr><tr><td>val_mse</td><td>█▆▂▂▃▁▁▁▁▁▃▂▂▂▁▁▃▁▁▁▁▁▃▁▂▁▂▂▃█▂▃▃</td></tr><tr><td>val_r2</td><td>▁▄▇▇▆█████▆▇▇▇██▆█████▆█▇█▇▇▇▃▇▇▇</td></tr><tr><td>val_rmse</td><td>█▅▂▂▃▁▁▁▁▁▃▂▂▂▁▁▃▁▁▁▁▁▃▁▂▁▂▂▃▇▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00433</td></tr><tr><td>trainer/global_step</td><td>4448</td></tr><tr><td>val_loss</td><td>0.00168</td></tr><tr><td>val_mae</td><td>0.00646</td></tr><tr><td>val_mape</td><td>0.00645</td></tr><tr><td>val_mse</td><td>8e-05</td></tr><tr><td>val_r2</td><td>-0.02366</td></tr><tr><td>val_rmse</td><td>0.00881</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-resonance-9</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/awbn84uf' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/awbn84uf</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140459-awbn84uf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:06:16,240] Trial 3 finished with value: 0.001683520502410829 and parameters: {'learning_rate': 0.0006377580927881986, 'num_layers': 2, 'num_heads': 4, 'hidden_size_multipliers': 15, 'dropout_prob': 0.30083757076604656}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c359c5c34c2a44fda88a9b7f21892780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011133692599999752, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140616-8l2kas0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8l2kas0w' target=\"_blank\">upbeat-snowball-10</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8l2kas0w' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8l2kas0w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 1.0 M \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.104     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d805959748e45abbda70130b2b1909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▂█▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>▃█▃▁▃▁▁▁▂▃▁▁▂▁▁▂▂</td></tr><tr><td>val_mape</td><td>▃█▃▁▃▁▁▁▂▃▁▁▂▁▁▂▂</td></tr><tr><td>val_mse</td><td>▂█▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▇▁▇█▇███▇▇███████</td></tr><tr><td>val_rmse</td><td>▃█▃▁▃▁▁▁▂▃▁▁▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00013</td></tr><tr><td>train_loss_step</td><td>0.00907</td></tr><tr><td>trainer/global_step</td><td>2224</td></tr><tr><td>val_loss</td><td>0.00246</td></tr><tr><td>val_mae</td><td>0.00865</td></tr><tr><td>val_mape</td><td>0.00871</td></tr><tr><td>val_mse</td><td>0.00012</td></tr><tr><td>val_r2</td><td>-0.68959</td></tr><tr><td>val_rmse</td><td>0.01087</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-snowball-10</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8l2kas0w' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/8l2kas0w</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140616-8l2kas0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:07:03,938] Trial 4 finished with value: 0.0024638778995722532 and parameters: {'learning_rate': 0.06461074121620844, 'num_layers': 2, 'num_heads': 8, 'hidden_size_multipliers': 14, 'dropout_prob': 0.11240434630188645}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7952b30d13a84c2da9208077be489310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011162918977776017, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140703-hlvkxmxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hlvkxmxt' target=\"_blank\">proud-dawn-11</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hlvkxmxt' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hlvkxmxt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 1.2 M \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.738     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83a2bce509d43b3b867c8693d4d71a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▂▃▂▃▂▄▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▄▃▆▃▄▃▄▂▄█▁▅▂▃▂▁▃▃▃▇▄▁▄▃▂▃▁▃▂▂▂▆▁██▂▆▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▅▆▇▃▁▃▃▅██▇▆▃▃▄▄</td></tr><tr><td>val_mae</td><td>▅▆▇▃▁▃▃▅██▇▆▃▃▄▄</td></tr><tr><td>val_mape</td><td>▅▅▇▃▁▃▃▅██▇▆▃▃▄▄</td></tr><tr><td>val_mse</td><td>▅▆▇▃▁▃▃▅██▇▆▃▃▄▄</td></tr><tr><td>val_r2</td><td>▄▃▂▆█▆▆▄▁▁▂▃▆▆▅▅</td></tr><tr><td>val_rmse</td><td>▅▆▇▃▁▃▄▅██▇▆▃▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00101</td></tr><tr><td>trainer/global_step</td><td>2085</td></tr><tr><td>val_loss</td><td>0.00178</td></tr><tr><td>val_mae</td><td>0.00671</td></tr><tr><td>val_mape</td><td>0.00669</td></tr><tr><td>val_mse</td><td>9e-05</td></tr><tr><td>val_r2</td><td>-0.10134</td></tr><tr><td>val_rmse</td><td>0.00909</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-dawn-11</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hlvkxmxt' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hlvkxmxt</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140703-hlvkxmxt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:07:59,713] Trial 5 finished with value: 0.001782930106855929 and parameters: {'learning_rate': 0.0070325091803032334, 'num_layers': 3, 'num_heads': 8, 'hidden_size_multipliers': 11, 'dropout_prob': 0.12356704492219633}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5c9a2599a540f5bcd6124080aaf9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011116819900000035, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140759-hd8i5ca4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hd8i5ca4' target=\"_blank\">gallant-dragon-12</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hd8i5ca4' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hd8i5ca4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 26.9 K\n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "26.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.9 K    Total params\n",
      "0.108     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc5c267698749629607e3a26c0dc6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.14902102973168962, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▃▂▁▂▂▂▁▃▂▂▂▂▂▂▃▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▂▁▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mape</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_rmse</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00017</td></tr><tr><td>train_loss_step</td><td>0.0094</td></tr><tr><td>trainer/global_step</td><td>9730</td></tr><tr><td>val_loss</td><td>0.00178</td></tr><tr><td>val_mae</td><td>0.00671</td></tr><tr><td>val_mape</td><td>0.00669</td></tr><tr><td>val_mse</td><td>9e-05</td></tr><tr><td>val_r2</td><td>-0.10341</td></tr><tr><td>val_rmse</td><td>0.00909</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-dragon-12</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hd8i5ca4' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/hd8i5ca4</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140759-hd8i5ca4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:09:37,914] Trial 6 finished with value: 0.0017820895882323384 and parameters: {'learning_rate': 1.2491318781595989e-05, 'num_layers': 1, 'num_heads': 2, 'hidden_size_multipliers': 3, 'dropout_prob': 0.24541517213391278}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287b99d70d3146d39db8dcdf246451b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011159983333333104, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_140937-xi5ez7cy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/xi5ez7cy' target=\"_blank\">unique-deluge-13</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/xi5ez7cy' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/xi5ez7cy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 625 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "625 K     Trainable params\n",
      "0         Non-trainable params\n",
      "625 K     Total params\n",
      "2.503     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3edeeb7cab48a69569a150a2101bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>hit_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▂▃▃▂▄▄▃▂▃▃▄▃▃▁▂▂▁▂▃▂▃▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▄▃▂▂▂▁▁▁▂▃▁▂▂▂▁▃▃▃▂▁▂▂▁▂▁▁▁▂▄▃▂▁█▃▁▁▂▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▇▆▆▅▆▇▇██▅▅▅▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>▇▆▆▅▆▇▇██▅▅▅▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mape</td><td>▇▆▆▅▆▇▇██▅▅▅▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>▇▆▆▅▆▇▇██▅▅▅▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▂▃▃▄▃▂▂▁▁▄▄▄▄▅▆▆▇██████████████</td></tr><tr><td>val_rmse</td><td>▇▆▆▅▆▇▇██▅▆▅▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>hit_rate</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00012</td></tr><tr><td>train_loss_step</td><td>0.00365</td></tr><tr><td>trainer/global_step</td><td>4212</td></tr><tr><td>val_loss</td><td>0.00169</td></tr><tr><td>val_mae</td><td>0.00649</td></tr><tr><td>val_mape</td><td>0.00648</td></tr><tr><td>val_mse</td><td>8e-05</td></tr><tr><td>val_r2</td><td>-0.02919</td></tr><tr><td>val_rmse</td><td>0.00882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-deluge-13</strong> at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/xi5ez7cy' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/xi5ez7cy</a><br/> View project at: <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_140937-xi5ez7cy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 14:11:08,057] Trial 7 finished with value: 0.0016880895709618926 and parameters: {'learning_rate': 0.014157065500477062, 'num_layers': 3, 'num_heads': 8, 'hidden_size_multipliers': 6, 'dropout_prob': 0.2740561789751887}. Best is trial 1 with value: 0.0016582789830863476.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8517e2b944bc42c898f75e953d0bae61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011164535644444288, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frederikmohr/masterarbeit/NN_Forecasting/NN_regression/wandb/run-20240602_141108-lbffawhh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/lbffawhh' target=\"_blank\">faithful-star-14</a></strong> to <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/lbffawhh' target=\"_blank\">https://wandb.ai/frederik135/cyclical_hyperparameter_tuning_transformer/runs/lbffawhh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TransformerModel | 103 K \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "103 K     Trainable params\n",
      "0         Non-trainable params\n",
      "103 K     Total params\n",
      "0.413     Total estimated model params size (MB)\n",
      "/opt/anaconda3/envs/masterarbeit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from models import TransformerModel\n",
    "from config import num_features, model_config, device, seq_length, architecture, wandb_config, num_epochs\n",
    "from preprocessing import train_loader, val_loader, test_loader, label_scaler\n",
    "import wandb\n",
    "\n",
    "class StockPredictionModule(LightningModule):\n",
    "    def __init__(self, model, label_scaler, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.label_scaler = label_scaler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seqs, labels = batch\n",
    "        y_pred = self(seqs)\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        labels_rescaled = self.label_scaler.inverse_transform(labels.reshape(-1, 1)).flatten()\n",
    "        predictions_rescaled = self.label_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        r2 = r2_score(labels_rescaled , predictions_rescaled)\n",
    "        mse = mean_squared_error(labels_rescaled , predictions_rescaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(labels_rescaled , predictions_rescaled)\n",
    "        mape = np.mean(np.abs((labels_rescaled  - predictions_rescaled) / (predictions_rescaled + 1e-8)))\n",
    "        pct_change_labels = [label for label in labels_rescaled]\n",
    "        pct_change_predictions = [prediction for prediction in predictions_rescaled]\n",
    "        hit_rate = np.mean(np.sign(pct_change_labels) == np.sign(pct_change_predictions))\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_r2\", r2, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mse\", mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mae\", mae, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_mape\", mape, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"hit_rate\", hit_rate, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_r2\": r2, \"val_mse\": mse, \"val_rmse\": rmse, \"val_mae\": mae, \"val_mape\": mape, \"hit_rate\": hit_rate}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])\n",
    "    hidden_size_multipliers = trial.suggest_int('hidden_size_multipliers', 1, 16)\n",
    "    hidden_size = num_heads * hidden_size_multipliers\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "\n",
    "    if hidden_size % num_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned(\"hidden_size is not divisible by num_heads\")\n",
    "\n",
    "    model_config.update({\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout_prob\n",
    "    })\n",
    "\n",
    "    wandb_config = {\n",
    "        \"architecture\": architecture,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout_prob,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": num_epochs\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"cyclical_hyperparameter_tuning_transformer\", entity=\"frederik135\", config=wandb_config, reinit=True)\n",
    "    model = TransformerModel(**model_config).to(device)\n",
    "    module = StockPredictionModule(model=model, label_scaler=label_scaler,\n",
    "                                   train_loader=train_loader, val_loader=val_loader, test_loader=None)\n",
    "    module.hparams.learning_rate = learning_rate\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        accelerator = \"gpu\"\n",
    "        devices = 1\n",
    "    elif torch.backends.mps.is_built():\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    else:\n",
    "        accelerator = None\n",
    "        devices = None\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"cyclical_hyperparameter_tuning_transformer\", log_model=\"all\", config=wandb_config)\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=70,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    val_result = trainer.validate(module, dataloaders=val_loader, verbose=False)\n",
    "    val_loss = val_result[0].get('val_loss', float('inf'))\n",
    "    wandb.finish()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
